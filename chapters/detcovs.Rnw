\chapter{Covariates in encounter rate and detection probability models}
\label{ch:detcovs}

\abstract{This chapter concerns the effect of covariates on encounter rates and detection probabilities. It shows how to model their dependence survey duration and survey effort, on detector, occasion and session, on individual-level covariates (behavioural response and sex, for example), and importantly, on spatial covariates. It also deals with the interpretation of models fitted with these kinds of dependence.}

\todo[inline]{Needs proper introduction.

Mention distance as an explanatory variable that is always used in SCR.}

\section{Introduction}

Due to the inherent variability of natural systems, SCR model parameters describing the detection process are likely to vary in space, time, among individuals, and posibly in response to detection. Modeling this heterogeneity adds complexity and requires larger samples than would otherwise be needed, but ignoring it can cause biased estimation and flawed inference.
Unexplained heterogeneity in detection probabilities, especially among individuals, has always been regarded as problematic in CR studies. With SCR, encounter rate or detection probability is alwasys a function of the distance between each individual's activity center and detectors, so the models explicitly account for one important source of heterogeneity among indivduals: spatially variable exposure to detectors. 
This chapter describes how other sources of heterogeneity can be modelled using covariates. Early ML SCR models accommodated covariates specific to detector locations, sampling occasions, sessions, and individuals. Extensions allowed indivdual heterogeneity to be modelled using unobserved or partially-observed covariates, and effective distances between activity centers and traps to be modified to account for habitat effects.

An evaluation of detection heterogeneity begins with a set of candiate models with different sources and/or patterns of variation in detection parameters. MS criteria can then be used to evaluate support for specific covaraites and models.
Estimation of population density is the goal of most SCR studies; parameters related to the detection process are often termed "nuisance" paramters. However, models of the detection process represent biological hypotheses, including regarding differences in home range size, which may also be of interest.  

Sample size & heterogeneity (as a subsection?): With sparse data there is less power both to detect, and to model, heterogeneity in detection parameters. If heterogeneity affects the data collected, but het not detected --> biased but apparently precise. If detectable, need sufficient data to inform estimation of the effect, variances and CIs can become too wide to be useful. Further increases in sample size --> more forms of het detected & must be modelled... but tapering effects, different models that include most important effects yield similar estimates. With very rich data different models yield the same accurate result (Tai chimp study). With sparse data simple models yield conservative estimates (IH not modelled)?

We begin by explaining how SCR parameters can be, and are, are expressed as functions of covariates <intercep/scale & link function). We then present and demonstrate methods for modelling increasingly complex covariate effects.

\section{Link function formulation}

\todo[inline]{hopefully the intro will remedy this; the different formulations are pretty deep water for the uninitiated, readers need a motiviation for why they are being taken through this.  Why not just a single formulation; is one better than the other?}
\todo[inline]{Introduce "conventional" intercept; scale form first and emphasize it, note only as an aside that ling function; linear predictor is also possible?}
The GLM \{link function; linear predictor\} form for modelling detection probability is commonplace in non-spatial capture-recapture literature. This formulation uses a limited number of ``link function'' forms to link a linear function of the parameters (the ``linear predictor'') to the expected value of the dependent variable \todo[inline]{Greg:
sorry if I am being pedantic but doesn't ordinary GLM's link the linear predictor to a parameter of interest rather than the expected value (which is obviously equal to theparameter ion the Poisson case)? I'm thinking about logistic regression where the linear predictor is linked to p]}(expected encounter rate or detection probability in the SCR case). For example, expected encounter rate $\lambda(x)$, and detection probability $p(x)$, could be modelled as functions of a covariate $x$ using a linear predictor $\beta_0 + \beta_x x$ with a ``log link'' for expected encounter rate, and a ``complimentary log-log link'' for detection probability, thus:
\be
\lambda(x)&=&e^{\beta_0 + \beta_x x} \nonumber \\
p(x)&=&1-\exp\left\{-e^{\beta_0 + \beta_x x}\right\}.
\label{eq:link-lp}
\ee

An alternative form is commonly used in the distance sampling literature. This is characterised by separately modelling an intercept term and the scale parameter of a distance-dependent function that has intercept equal to 1. For brevity, we will refer to this as the \{intercept; scale\} form. This form has been adopted by many SCR models, where the expected encounter rate is modelled using an intercept term $g_0$ or $\lambda_0$, and a distance-dependent ``key function'' or ``kernel'' with a scale parameter $\sigma$. In Eqn~\eqref{eq:ER+detfun.p.hn}, for example, the intercept is $g_0$ and the key function or kernel is $\exp\{-d^2/(2\sigma^2)\}$, while in Eqn~\eqref{eq:ERdetfun-hr}, the intercept is $\lambda_0$ and the key function is $[1-\exp\{-(d/\sigma)^b\}]$. 
\todo[inline]{Could we specify here that these are the Gaussian or half-normal and hazard rate key functions, respectively, for clarity?. Also the text above these equations in Ch 2 uses "range parameter" rather than "scale parameter")
In both cases $d$ is distance from a detector to an activity centre. (The parameter $\sigma$ is a ``scale parameter'' because it ``scales'' $d$ by dividing it.)

Because SCR capture history likelihoods are just products of Poisson (in the case of count data) or Bernoulli (in the case of binary data) likelihoods, one might expect that the Poisson mean ($\lambda(d)$) and Bernoulli mean $(p(d)$) would be linked to the explanatory variable $d$ using the GLM inverse link functions for these two distributions (the inverse log link, and the inverse logit or cloglog link functions, respectively -- see Table~\ref{tab:ERdetfun-invlinks}). 

This is the case for only some encounter rate and detection function forms that are commonly used in SCR models: some can be written in GLM \{link function; linear predictor\} form, but not all can be. 

The Gaussian model for expected encounter rate (Eqn~\ref{eq:ER+detfun.lambda.hn}) is and example in which the mean encounter rate is readily formulated like this. It can be written using a log link function as
\be
\lambda(d)\;=\;\lambda_0\exp\left\{-\frac{d^2}{2\sigma^2}\right\}&=&\exp\{\beta_0\}\exp\{\beta_d d^2\} \nonumber \\
&=&\exp\{\beta_0 + \beta_d d^2\}\;=\;e^{\eta}
\label{eq:ERdetfun-gaussian-lambda}
\ee
\noindent
where $\lambda_0=\exp\{\beta_0\}$, $\beta_d=-1/(2\sigma^2)$, and $\eta=\beta_0 + \beta_d d^2$ is the linear predictor. An obvious alternative is to replace $d^2$ wtih $d$ as the explanatory variable:
\be
\lambda(d)&=&\exp\{\beta_0 + \beta_d d\},
\label{eq:ERdetfun-negexp-lambda}
\ee
\noindent
and this gives a negative exponential form for the encounter rate function, which we see can also be written in GLM \{link function; linear predictor\} form. 

The hazard rate form for mean encounter rate (Eqn~\ref{eq:ERdetfun-negexp-lambda}) WRONG REF HERE AND BELOW? cannot be written in this form, while the hazard rate from for detection probability can only be written in this form if $g_0=1$ (with an inverse cloglog link and linear predictor $\beta_0 + \beta_d\log(d)$, in which $\beta_0=b\log(\sigma)$ and $\beta_d= -b$). The Gaussian detection function model cannot be written in this form. 

%If we can write the encounter rate function as $\lambda(d)=e^{\eta}$, where $\eta$ is a linear predictor (i.e. a linear function of parameters), then the detection function can be expressed in \{link function; linear predictor\} form using the cloglog link function. This provides a natural way of incorporating survey effort into detection probability functions (see Section~\ref{sec:ERdetfund-effort}). If we combine $\lambda(d)=e^{\eta}$ with Eqn~\eqref{eq:ER+detfun.binaryp} (and use $T=1$ for simplicity), we get
%\be
%p(d)&=&1-\exp[-\exp\{\eta\}] 
%\label{eq:ERdetfun-cloglogp(d)}
%\ee
%\noindent
%which links the detection probability, $p(d)$, to a linear predictor $\eta$ with the inverse cloglog link function.

When the intercept of an encounter rate model that can be written using a log link, is made to depend on covariates using a log link, this just adds a term to the linear predictor. For example, if we make $\lambda_0$ depend on a covariate $z$ thus: $\lambda_0(z)=\exp\{\beta_0+\beta_z z\}$, then Eqn~\eqref{eq:ERdetfun-negexp-lambda} becomes $\lambda(d,z)=\exp\{\beta_0+\beta_d d^2 + \beta_z z\}$. 

But when the scale parameter $\sigma$ in an \{intercept; scale\} model is made to depend on covariates, it is not possible to write this in the standard \{link function; linear predictor\} form. For example, making $\sigma$ a function of covariates in Eqn~\eqref{eq:ER+detfun.p.hn} or Eqn~\eqref{eq:ERdetfun-lambda-hr} amounts to making the parameter $\beta_d$ a function of the covariates (because $\beta_d=-1/(2\sigma^2)$), and having parameters that are functions of covariates does not fit in the standard GLM \{link function; linear predictor\} framework.

Unlike non-spatial CR models, SCR models tend to use the \{intercept; scale\} formulation for modelling expected encounter rate and detection probability. They do also tend to use a \{link function; linear predictor\} formulation, but they do this separately for the intercept and scale parameters, not for encounter rate or detection probability as a whole (see below).

\section{Modelling intercept and scale parameters}

\todo[inline]{this might better fit before the previous section--it is more understandable

Greg: definitely agree}

While there is no single correct way to model the dependence of encounter rate and detection probability on the covariates, it is commonly done
\bi
\item by making the intercept of the encounter rate function ($\lambda_0$) or the detection function ($g_0$) depend on covariates and/or
\item by making the scale parameter ($\sigma$) of the encounter rate function or the detection function depend on covariates.
\ei

\todo[inline]{Or both! Furthermore, it's not an arbitrary choice. Some covariates might not be appropriate to both parameters, and different covariate models represent hypotheses. We might use camera trap model as a covariate for the intercept but it wouldn't make sense as a covariate for the scale .. Point this out here?}  

These parameters are made to depend on covariates by expressing them as functions of the covariates. One might, for example, think of making the probability $g_0$ depend on a variable $z$ in this way: $g_0=\beta_0 + \beta_1 z$, where $\beta_0$ and $\beta_1$ are unknown parameters. But this would not be a good idea because there is nothing in this equation to stop $g_0$ going out of bounds (above 1 or below 0). Link functions are used to keep parameters within their bounds. In a similar way that a generalised linear model (GLM) link function relates a ``linear predictor'' (often denoted $\eta$, and equal to $\beta_0 + \beta_1 z$ in the example above) to the expected value of a response variable, link functions in SCR analyses link a linear predictor to a parameter of interest ($g_0$ in this example).
In the case of $g_0$ we need a link function that keeps it between 0 and 1, while in the cases of $\lambda_0$ (a rate, and hence bounded below by zero but unbounded above) and $\sigma$ (also bounded below by zero but unbounded above) we need a link function that keeps them non-negative. Common choices are a logit link or complimentary log-log link (``cloglog'' link for short) for $g_0$, and log links for $\lambda_0$ and $\sigma$. 
\todo[inline]{These two paragraphs should preceed most of the discussion of link functions, either by removing this paragraph there, or reversing the order of the sections.}

By convention, link functions specify $\eta$ as a function of the parameter of interest ($g_0$, $\lambda_0$ or $\sigma$ here). But it is easier to interpret the inverse link functions, which express $g_0$, $\lambda_0$ or $\sigma$ as functions of the linear predictor $\eta$. Examples of the inverse link functions of a linear predictor $\eta=\beta_0+\beta_1 z$ are shown for positive and negative $\beta_1$ in Figure~\ref{fig:ERdetfun.linkfuns}. Notice that the shapes of the inverse logit and inverse cloglog functions are very similar, which makes the choice between the two relatively unimportant in SCR analyses.

The equations for the inverse link functions illustrated in Figure~\ref{fig:ERdetfun.linkfuns} are as shown in Table~\ref{tab:ERdetfun-invlinks}.

\begin{center}
\begin{table}
\caption{Inverse link functions for $g_0$, $\lambda_0$ and $\sigma$., with linear predictor $\eta=\beta_0 + \sum_{m=1}^M z_m$, where $z_1,\ldots,z_M$ are $M$ different covariates.}
\label{tab:ERdetfun-invlinks}
\begin{tabular}{ rll } 
$g_0=\;$ & $\;\frac{e^\eta}{1+e^\eta}$ & (inverse logit link) \\
$g_0=\;$ & $\;1-\exp\left\{e^{-\eta}\right\}$ & (inverse cloglog link) \\
$\lambda_0=\;$ & $\;e^\eta$ & (inverse log link) \\
$\sigma=\;$ & $\;e^\eta$ & (inverse log link)
\end{tabular}
\end{table}
\end{center}


\begin{figure}[ht]
\caption{\small Illustrative inverse logit and cloglog (left) and log (right) link functions of a linear predictor $\eta=\beta_0+\beta_1 z$. Solid lines are for positive $\beta_1$ while dashed lines are for negative $\beta_1$. The left plot shows the inverse logit link in black and the inverse cloglog link in grey.}
\centering
\vspace{-24pt}
\includegraphics[width=12cm]{keepfigure/linkfuns.pdf}
\label{fig:ERdetfun.linkfuns}
\end{figure}

Dependence on more covariates is modelled by adding terms to the linear predictor: $\eta=\beta_0 + \beta_1 z_1 + \beta_2 z_2 + \cdots + \beta_m z_m$. 



\todo[inline]{Should we mention regression splines here, and add a plot explaining?}




\section{Detector-, occasion- and stratum-specific effects}
\label{sec:ERdetfun-dos-effects}

Incorporating detector-specific, occasion-specific and stratum-specific covariates in expected encounter rate or detection function models is done by simply adding terms to the appropriate linear predictor (after importing data like that shown in Table~\ref{tab:ERdetfun-detdata}). If one uses the \{intercept; scale\} form, then the intercept and the scale parameters can separately be made to depend on covariates, using the intercept-specific and scale-specific \{link function; linear predictor\} forms shown in Table~\ref{tab:ERdetfun-invlinks}. An alternative is to add the covariates to a \{link function; linear predictor\} model for the encounter rate or detection function itself. This is much less common in the SCR literature.
\todo[inline]{
Detector-specific covariates can remain the same at each detector for the survey duration, or vary with occasion and/or stratum (see Table~\ref{tab:ERdetfun-detdata}). In addition, encounter rate and detection function models can be made to depend on occasion and/or stratum in the same way for all detectors, by using occasion number or stratum name as covariates. 

\subsection{Example analyses}

\todo[inline]{David's initial thoughts: (1) Show data structure and importing; (2) stuff below was written before divison of labour clarified - use or not, as you see fit. (3) There is potentially a LOT to cover here: (a) detector-specific covariates (e.g. sname in Kruger data, I think), (b) occasion-specific covariates (not possible with Kruger data because we have only one occasion - we could simulate multiple occasions; should we do this?), (c) Session-specific covariates (not possible with Kruger data, but we should really demonstrate the "Session" vs "session" option here somehow, shouldn't we?), (d) time-varying, detector-specific covariates (? - not sure if necessary - ?)}

The \texttt{secr} syntax for this follows the syntax for GLMs in \texttt{R}:
\noindent
{\small
\begin{svgraybox}
\texttt{
g0 \url{~} z1 + z2 + $\cdots$ + zm \\
lambda0 \url{~} z1 + z2 + $\cdots$ + zm \\
sigma \url{~} z1 + z2 + $\cdots$ + zm 
}
\end{svgraybox}
}
\todo[inline]{Not sure why this box appears here. Tildes and brackets missing? Specify "formula notation" in R, and note that z1 represents "base levels" of covariates?}
\noindent
The \texttt{secr} package automatically selects the log link for $\sigma$ and $\lambda_0$, and the logit link for $g_0$. Interaction terms and main effects with interactions can be added using ``\texttt{:}'' and ``\texttt{*}'', respectively, as with GLMs in \texttt{R}.
\todo[inline]{I'm sure readers would be pleased to read this near the end of the section on link functions}

\subsection{Effort effects}
\label{sec:ERdetfund-effort}
\todo[inline]{struggling to see how an ecologist will use this info}
\todo[inline]{Eric: in response to the comment above, I've added some of what I think ecologists will want to know about modeling effort.}

As we have seen, SCR models are parameterized in terms of the duration of samplnig at specific locations. Sampling effort at each location must therefore be accurately quantified for the models to yield accurate results. If the input data overestimate sampling effort, density estimates will be negatively biased, and vice versa.  
Any time a detector is moved or fails to function, the sampling duration at the associated locations is less than the survey duration and must be specified in the model. SCR models allow us to specify, for each location where a detector operated: (1) the total duration of sampling, (2) whether or not it operated for the entirety of each occasion, or (3) the duration of sampling on each occasion, depending on the type of detector (see Ch 5) and the information available.   
In the case of area searches we specify the specific areas searched and the number of times they were searched. More detail + transects to be added here.
In some cases, accurately quantifying effort may requrie that some capture data are excluded from the analysis. Suppose that, upon arrival at a barbed wire corral that is typically capable of sampling multiple individuals during a single occasion, a researcher finds that the detector was rendered inoperable since the last visit to the site 1 week ago (a tree has fallen across the trap site, knocking the wire to the ground), but there are hair samples on the wire which can only have been snagged before the tree fell. We might still be able to quantify effort accurately (e.g., the tree certainly fell during the storm between 5 and 6 p.m. on Wednesday). However, if we cannot know how long the detector operated, it should be considered to have been inoperable for the entire occasion, and the samples excluded from the capture data. Similarly, if electronic detectors that record times of detection (camera traps or accoustic detectors) stop operating between visits by a researcher, using the last time of detection of the target species as the time at which the detector failed would likely underestimate effort. It might be possible to infer the approximate time of failure if the device frequently detected non-target species or background noise, otherwise data collected since the previous visit by a researcher might need to be censored, and the detector considered inoperable over that time. If a search of a defined area must be terminated before it can be searched completely (another storm), the area should be redifined as the area searched, or the partial search and any associated detections should be excluded from the data. 

Sampling effort at each location is conveniently modeled as a detector-level covariate. \todo[inline]{Make sure it's clear that "detector-level" covariates are specific to locations, not e.g. sampling devices or observers, which would be covariates of the location}. 
In 'secr', sampling effort at each location is defined as an attribute of the traps object, and therefore of the capture history object. \todo[inline]{Remove below with examples?}

Under the assumption of independent detections of individuals, the relationship between effort ($T$) and expected number of encounters is obvious: if you double effort, you double the expected number of encounters; if you quarter the effort you quarter the expected encounter rate, and so on. 

\todo[inline]{Greg:

$T$ is already used for survey duration, is the idea here to initially think of effort ito duration rather than in a generic sense?. Also, isn't the proportionality premised on a constant hazard? Maybe this should be mentioned here and then generalised in the CT chapter.}
\todo[inline]{
More succinctly: $\lambda(d,T)=\lambda(d,1)T=\lambda(d)T$, where $\lambda(d)=\lambda(d,1)$ is the expected encounter rate per unit effort. This is true if effort is time, as we saw earlier in this chapter, but it is also true for any kind of effort in which each unit acts on encounter rate independently of other units. For example if you have a cluster of detectors, each detecting independently, then each individual detector can be viewed as one unit of effort. (Because we focus on effort in this section, we drop explicit dependence on distance when writing $\lambda$ and $p$ below.)

Each additional unit of independent effort adds one ``expected encounter rate unit''. As a result, when effort comes in discrete units (like the individual detectors in an array) the total expected encounter rate of the array ($\lambda_{ijk}(C_{jk})$ for individual $i$ with the $j$th cluster $C_{jk}$ detectors on occasion $k$) is the sum of the individual encounter rates ($\lambda_{ijk}$ for detector $j$ in the cluster):
\be
\lambda_{ijk}(C_{jk})&=&\sum_{j=1}^J\lambda_{ijk}\;=\;\lambda_{ijk}C_{jk}
\label{ERdetfn-ertot-C}
\ee
When effort is a continuous measure, like time, summation is replaced by integration, and if the expected encounter rate is constant at $\lambda_{ijk}$ over the time period $T_{jk}$:
\be
\lambda_{ijk}(T_{jk})&=&\int_0^{T_{jk}}\lambda_{ijk}\;=\;\lambda_{ijk}T_{jk}
\label{ERdetfn-ertot-T}
\ee
Now if
\be
\lambda_{ijk}&=&e^{\eta_{ijk}}
\ee
\noindent
and we use $T_{jk}$ to denote generic effort ($C_{jk}$ or $T_{jk}$ in the above two examples), then we can write Eqns~\eqref{ERdetfn-ertot-C} and \eqref{ERdetfn-ertot-T} as
\begin{svgraybox}
\be
\lambda_{ijk}(T_{jk})&=&e^{\eta_{ijk}+\log(T_{jk})}.
\label{eq:ERdetfun-ER-logoffset}
\ee
\end{svgraybox}
This describes how effort affects expected encounter rate. How does it affect detection probability? To see this, we use Eqn~\ref{eq:ER+detfun.lambda.to.p}:
\begin{svgraybox}
\be
p_{ijk}(T_{jk})&=&1-\exp\{-\lambda_{ijk}T_{jk}\}\;=\;1-\exp\left\{-e^{\eta_{ijk}+\log(T_{jk})}\right\}.
\label{eq:ERdetfun-p-logoffset}
\ee
\end{svgraybox}
Notice that if we set $T_{jk}=0$ this makes the expected encounter rate and the detection probability zero. We can therefore use a binary effort variable ($T_{jk}=1$ if detector $j$ was operating on occasion $k$ and $T_{jk}=0$ otherwise) to ``switch'' detectors on and off in an SCR analysis.

In the case of both expected encounter rate functions and detection probability functions, effort, $T_{jk}$, enters as an offset $\log(T_{jk})$ in the linear predictor. This makes effort a different kind of covariate than others, which would normally enter the linear predictor in linear form with an unknown parameter: $\eta_{ijk}+\beta_T T_{jk}$. This form results in expected encounter rate being proportional to $\exp\{\beta_T T_{jk}\}$, not $T_{jk}$. When effort units act independently this is not an appropriate relationship between effort and expected encounter rate. It corresponds to each additional unit of effort causing greater increase in the expected encounter rate that the previous additional unit, or an ``acceleration'' in the effort effect. 

Suppose that we have a detection function $p_{ijk}^*=p_{ijk}(T_{jk}^*)$ that was specified to have a certain form (Gaussian/hazard-rate/inverse cloglog/...) for some default effort level, $T_{jk}^*$. What is the detection function for some other effort level $T_{jk}$? This is easy to specify in terms of expected encounter rates because this is proportional to effort: $\lambda_{ijk}(T_{jk})=\lambda_{ijk}(T_{jk}^*)\times\frac{T_{jk}}{T_{jk}^*}$. And from Eqn~\eqref{eq:ER+detfun.p.to.lambda}: $\lambda_{ijk}(T_{jk}^*)=-\log(1-p_{ijk}^*)/T_{jk}^*$. Hence
\begin{svgraybox}
\be
p_{ijk}(T_{jk})&=&1-\exp\left\{\frac{T_{jk}}{T_{jk}^*}\log(1-p_{ijk}^*)\right\}.
\label{eq:ERdetfun-p2pT}
\ee
\end{svgraybox}


\todo[inline]{Maybe need to cover effort in multi-catch trap scenario when we deal with mult-catch traps?}

\subsection{Example variable-effort analyses}

\todo[inline]{David's initial thoughts on this: Show data structure and importing here; Note difference between continuous and binary effort. Workshop example shows spatial effect of ignoring effort, which is fine, but does mean we have to tell them about density models and how to specify these in secr - I had hoped we could delay that until after dealing with spatial models. Might be worth ignoring the spatial model in this example (at least in fitting) and comparing the AICcs and estimates with and without effort? That makes things simpler.(And we could give the problem with spatial model as an exercies/example in a later chapter?) Also, having mentioned effort entering as a log-linear effect rather than offset, I wonder if it is worth looking at this model in the example? (Depends on what results it gives!) 
Alternatively, the wolverine data (see next Example box) might be useful here?}
\todo[inline]{Eric: I agree that we should keep density constant here. I don't agree with using AICc to show that modelling effort provides a better fit. That's fine for other covariates, which are usually hypothesized effects, but shouldn't effort always be accurately quantified in the model? Regarless of what AICc says? Modelling effort doesn't add parameters does it?
Also, the Tai chimp study is now in press. We could ask for permission to use those data. It could be useful for e.g. detection heterogeneity and sampling effort. There, cameras were frequently moved, some cameras were disabled or distroyed by wildlife. Only one detection per individual per location per day was included in the data, and the data were summarized into week-long occasions (# detections /indiv /loc / ocn binomially-distributed with size=7). We only included data from cameras that operated for complete occaions, but we could also parameterize effort as days-within occasions.} 

\section{Individual effects (trap response and heterogeneity)}

\todo[inline]{Insert text about bias from unmodelled heterogeneity.}
If encounter rates or detection probabilities vary among the individuals in a population, CR and SCR models that assume common detection probabilities yield biased estimates. If the heterogeneity is induced by detection, such that previously-detected individuals have different encounter rates or detection probabilities than undetected individuals (often termed behavioural responses to detection, or trap responses), the direction of bias depends on the direction of the response. If encounter rates or detection probabilities increase after initial detection (positive, or "trap-happy" responses), and the effect is not modelled, abundance is underestimated. If they decrease (negative or "trap-shy" responses), abundance is overestimated. If the differences among individuals are inherent, with some members of the population always having higher encounter rates or detection probabilities than others, regardless of whether or not they are detected, assuming common detection probabilities always causes underestimation of abundance.  

The difficulty with most covariates that are attached to individuals is that we don't know them for individuals we did not detect. It is useful to classify the individual-level covariates according to which individuals have known and which have unknown covariates, as follows:
\ben
\item Covariates that are known for all individuals at all times,
\item Covariates that are known for no individuals.
\item Covariates that are known for some individuals (usually those that were captured at least once) at all times,
\item Covariates that are known only for the captured individuals and only at the times of their capture, and
\een

Except in the case of the first item above, the values of some individual-level covariates in the population of interest are unobserved, and are therefore latent variables. The way we deal with latent variables is by modelling their probability distribution. This adds complexity, so let's deal with the first item above first. What is this covariate that we know for individuals that we never detect?
\todo[inline]{Not sure if this lead-in is still effective if we introduce trap responses above}

\subsection{Trap response}

Behavioural response to capture or detection can be modelled using an indicator variable that indicates whether or not an individual has been detected previously. If we define it to be $b_{ik}=1$ if individual $i$ was detected at least once before occasion $k$ and $b_{ik}=0$ if not, then each individual has a vector $\bm{b}_i=(b_{i1},\ldots,b_{iK})$ attached to it, and we know that for individuals that were never detected, this is a vector of zeros. To model a behavioural effect (``trap happiness'' or ``trap shyness'') we just add this covariate to the linear predictor at the relevant level.

For example, suppose that we are currently using a linear predictor $\eta_{ijk}$ for a model in which individuals have no behavioural response to being detected (or captured) and we want to investigate whether detection probability changes after first capture. We could fit a model with linear predictor
\be
\eta_{ijk}^*&=&\eta_{ijk} + \beta_b b_{ijk}
\label{eq:ERdetfun-eta_b}
\ee
\noindent
and if $\beta_b$ was significantly different from zero, we would conclude that detection probability does change after first capture. If the estimate $\hat{\beta}_b$ was negative we would conclude that animals became trap-shy, while if it was positive we would conclude that they became trap-happy.

Depending on the model, the link function in Eqn~\eqref{eq:ERdetfun-eta_b} might be that for a detection function with cloglog link, or for $g_0$ (if modelling detection probability) or $\lambda_0$ (if modelling encounter rate) and/or or $\sigma$ (see Table~\ref{tab:ERdetfun-invlinks}).

You can use $\bm{b}_i$ to model more complicated trap-response too. You might, for example, model the response as being transient - with the effect of capture on one occasion lasting only until the next capture occasion. This kind of model can be implemented by simply setting $b_{ik}=1$ only if the individual was captured on the previous occasion ($k-1$), and setting $b_{ik}=0$ otherwise. Various other more complex models are possible too, in which memory of capture fades with time since last capture. 

You might also want behavioural response to depend on detector, i.e., consider models in which individuals have a learned response not to detectors in general, but only to the detectors at which they have been previously captured. To do this, we just construct a covariate $b_{ijk}$ that is 1 only if individual $i$ was captured at detector $j$ prior to occasion $k$ (for a permanent behavoural response), or is 1 only if the individual was captured there on occasion $k-1$.
\todo[inline]{I think we should go with detector location, sample location, or location, rather than detector, throughout this chapter. This makes is sound as though the response could depend on the type of detector, but really it's the location.}

\subsubsection{Example analyses}

\todo[inline]{David's initial thoughts: Demonstrate models (in secr-speak): b, B, bk and Bk. (Not so sure about k and K, which seem a bit weird to me - can you think of situations in which they would be appropriate?). Andy's 2011 paper wolverine data are one example dataset that we could use (they used bk, I think). The data is in his scrbook package, available here: https://sites.google.com/site/spatialcapturerecapture/scrbook-r-package. I have the data from a previous version of the package and I have code that knocks it into secr-shape and fits some models, but I don't have a current version of the library, and it now only available as executable for Windows. I suspect the data has not changed.
\todo[inline]{Eric: bk seems more biologically realistic then b when traps are baited with a food reward, as in many carnivore studies. Even when there's no food reward, if an animal is wary of evidence of human disturbance, but unconcerned by a camera trap, it might react differently to camera trap placements it has an has not investigated previously). I worry that shifting activity centers within the larger home range could lead to support for bk models though (animal moves into an area midway through the study and is detected at a location for the first time, then recaptured because it remains in the area, not because of any trap response).}

The code is here: .../scrmlebook/inst/code/wolverine4secr.r and the data is here: .../scrmlebook/data/wch.prox.rda (formulated as binary proximity detectors) and here: .../scrmlebook/data/wch.count.rda (formulated as single-occasion count data),

I think it is quite a useful dataset, and it also has variable-effort.

Reference: Royle, J. A., Magoun, A. J., Gardner, B., Valkenburg, P. and Lowell, R. E. (2011) Density estimation in a wolverine population using spatial capture reecapture models. Journal of Wildlife Management 75, 604-611.}

\subsection{Unobserved covariates}
\todo[inline]{If we have a section called trap response, this should fall under one called Individual heterogeneity}

Covariates that affect encounter rate or detection probability but are not observed can be added to the linear predictors of these models just like covariates that are observed. The difficulty comes in evaluating the models. If we don't know what values these unobserved covariates take, what values should we use for them in the model? 

The answer is that we assign the values according to some probability model for the covariate and then evaluate the encounter rate or detection function models for every possible value of the unobserved covariates. This is called ``marginalising'' over the unknown covariate. The cost in terms of parameters is that we need additional parameters for the covariate probability model. We also of course need to specify a suitable probability model. 

There are two kinds of such model: probability density functions (PDFs), which treat the unobserved covariate as a continuous random variable (one that can take on an infinite number of values on the real line), and probability mass functions (PMFs) which treat it as a discrete random variable (one that can take on only a finite number of values). It is usually the case that you don't know what the unobserved covariates are (or at least you don't know what all of them are - there is usually more than one thing affecting encounter rate and detection probability). The choice between continuous and discrete latent random variables is usually therefore one of convenience and modelling preference.

Discrete latent covariate models in CR were first developed by \cite{Pledger:00}. Continuous models for latent capture probabilities were first developed by \cite{Burnham:72} and more recently by \cite{Dorazio+Royle:03}. There has been a lively debate about the advantages and disadvantages of each approach. \todo{Byron's approach?} The discrete covariate approach is usually called a ``finite mixture model'' and the approach developed for non-spatial CR by \cite{Pledger:00} was adapted by \cite{Borchers+Efford:08} for SCR. The continuous covariate approach is sometimes called an ``infinite mixture model''.\todo{Are there Bayesian SCR implementations?}

In the case of a discrete latent covariate $z$, we hypothesize that there are $h$ possible (unknown and unobserved) values $u_1,\ldots,u_h$ that the covariate can take and we assume a particular form for the PMF $f(z)$ from which these values are drawn. The most commonly-used PMF in this context for SCR models is a multinomial with $(h-1)$ parameters. This involves no assumptions about the shape of the PMF. If we use a two-part mixture model $h=2$, this leads to a Bernoulli PMF with a single parameter $\phi_1$. For $h>2$, we have a parameter vector of probabilities $\bm{\phi}_z=(\phi_1,\ldots,\phi_{h-1})$. (In all cases $\phi_h=1-\sum_{v=1}^{h-1}\phi_v$.)

As with any covariate, we model the effect of latent covariates via the relevant linear predictor(s) for $g_0$ or $\lambda_0$, $\sigma$ or the detection function if it is being modelled directly using a \{link function; linear predictor\} detection function form:
\be
\eta_{ijk}^*&=&\eta_{ijk} + \beta_z z_i
\label{eq:ERdetfun-eta_u}
\ee
\noindent
where $\eta_{ijk}$ is the linear predictor of a model for individual $i$ at detector $j$ on occasion $k$ that does not include the latent covariate $a_i$ for the individual, and $\eta_{ijk}^*$ is the linear predicture including the latent covariate.

Marginalising over the $z_i$s involves a weighted sum over $z_i=u_1,\ldots,u_h$ in which the weights are $f(u_1),\ldots,f(u_h)$. Details are given in Appendix~??\todo{insert math details}. 

In the case of continuous $z_i$ (an ``infinite mixture model''), $f(z)$ is a PDF on some interval on the real line, and it is parameterised by an unknown parameter vector $\bm{\phi}$. Marginalising over the $z_i$s in this case involves a weighted integral over the interval on the real line on which $f(z)$ is non-zero, with weight $f(z)$ at $z$. Details are given in Appendix~??\todo{insert math details}.

With both finite and infinite mixture models, including a latent covariate $z$ involves estimating the linear predictor parameter(s) $\beta_z$ associated with it for each linear predictor in the model, and the parameter vector $\bm{\phi}$ of the PDF or PMF $f(z)$. In the case of a finite mixture model with multinomial $f(z)$, the parameters $\phi_1,\ldots,\phi_h$ can be interpreted as the proportion of the population that has the (unobserved and unknown) latent covariate value $u_z,\ldots,u_h$ associated with them. And if we think of $u_z,\ldots,u_h$ as defining $h$ classes of individual, $\phi_1,\ldots,\phi_h$ can be interpreted as the proportion of the population in each class.

\todo[inline]{Maybe insert a plot of example PMF and PDF?}

\subsubsection{Example analyses}

\todo[inline]{David's initial thoughts: Note that secr only does finite mixture. Show how to use (in secr-speak) models h2 and h3, and choose between them using AIC. Consider them in $\lambda_0$ and/or in $\sigma$ Interpret the parameters and relate them to the algebraic parameters above (and if possible, explicitly to the linear predictor form above - maybe show how to calculate it from the estimates?). Hopefully, demonstrate that neglecting latent variables (i.e. allowing unmodelled heterogeneity) leads to lower density and abundance estimates, as is consistent with these estimates being negatively biased.

Dataset: Kruger data is an obvious candidate. Might be possible to continue wolverine dataset, just for continuity with previous example? But I don't know if Mh models are supported by the wolverine data.}

\subsection{Partially observed covariates}

Suppose now that we observe the sex of some individuals (perhaps all those we detect, but perhaps only some of those that we detect). In this case sex is a partially observed latent variable and although we know it for some individuals, we don't know it for all individuals and so we again need a probability model for the unobserved sexes, and we need to marginalise over sex for individuals whose sex we did not observe. 

We deal with this in the same way that we dealt with entirely latent variables, except that we marginalise over the latent variable $z$ (sex, in this example) only for those individuals for which $z$ is unknown. For the individuals with known $z$, we can evaluate the linear predictor(s) that include $z$ and we therefore don't need to marginalise over $z$ for them. Mathematical details are given in Appendix~??.\todo{Insert maths}.

Note that a model can have multiple partially-observed latent covariates (both sex and size, for example) in addition to an entirely latent covariate. You need to specify a PMF or PDF for each partially observed covariate as well as for an entirely latent covariate if that is in the model, and marginalising occurs over all latent covariates. (Actually, you need to specify the joint PMF or PDF of all partially observed and entirely latent covariates, but in practice this is most easily done by specifying a separate PDF or PMF for each and assuming independence between them.)

\subsubsection{Example analyses}


\todo[inline]{David's initial thoughts: Note that secr allows EITHER exactly one entirely latent covariate, OR exactly one partially-observed latent covariate, and in both cases only does the discrete variable case (finite mixture). Show how to use (in secr-speak) models h2 and h3 with argument hcov. Consider them in $\lambda_0$ and/or in $\sigma$ Interpret the parameters and relate them to the algebraic parameters above (and if possible, explicitly to the linear predictor form above - maybe show how to calculate it from the estimates?). Perhaps compare estimates from model that treates a variable (e.g. sex) as entirely latent, and one that treats it as partially observed? Might be a useful point in there about value of observing a variable - I am not sure. 

Dataset: Kruger data is an obvious candidate.Also possible to continue wolverine dataset, as this does have sex recorded for all detected individuals.}

\subsection{Time-varying unobserved or partially-observed covariates}

When entirely latent or partially-observed covariates change between occasions, things can become complicated! 

The simple case is when the covariate values are entirely latent and are independent on each occasion. In this case, there is probably no need to model the latent covariates at all. This is because unmodelled heterogeneity causes bias when it leads to detection and redetection (or capture and recapture) of the more detectable individuals across all occasions, and so detection probability estimates are biased towards the detectoin probability of the more detectable individuals\todo{Refer to (currently missing) unmodelled heterogeneity discussion above.}. But when detectability of any individual varies randomly across occasions, then detection probability on any one occasion is independent of detection probability on other occasions and this bias no longer applies.\todo{Need to think through carefully.}

The complicated scenario is when detection probability on one occasion is related to, but not necessarily equal to, detection probability on previous occasion(s). In this case we need a PDF or PMF for the latent covariate(s) that on any one occasion is related to, but not necessarily equal to, the PDF or PMF for the latent covariate(s) on previous occasions. If detection probability depends on an individual's weight, for example, we can't observe the weight on any occasion on which the individual is not detected, but its weight on these occasions will be related to any previously observed weights and any subsequently observed weights. 

?? ... ??\todo{Look up and insert citations} have developed non-spatial CR models that accommodate this kind of temporal dependence in partially-observed latent covariate values, but no such methods have as yet been developed for SCR. Use of Hidden Markov models to model temporal dependence would seem to be a promising approach to pursue. 


\todo[inline]{Need to mention pitfalls, especially with sparse data. Effects not detected --> biased but apparently precise. Detected--> wide confidence limits, possibly false minima & unrealistic estimates --> no reliable estimate.}

\section{Spatial effects}

Here we consider how to model the effect of spatially-referenced covariates on expected encounter rate and detection probability. A spatially-referenced covariate is covariate whose value depends on location. We can think of it as a function of location $\bm{s}$ and write it as $z(\bm{s})$. 

The expected encounter rate and detection probability always depend on location: usually, the closer the location of a detector to the location of an individual's activity centre, the higher the expected encounter rate and detection probability. In all of the models we have considered thus far, the effect of spatial location is captured only in the straight-line distance $d$ between detector and activity centre. In this case, the covariate that goes into encounter rate or detection probability models is a function of two locations, the location of the detector ($\bm{s}_k$ for detector $k$) and the location of the activity centre ($\bm{s}_i$ for individual $i$). Ignoring occasion and session for simplicity: $d_{ik}=d(\bm{s}_i,\bm{x}_k)=||\bm{s}_i-\bm{x}_k||=\sqrt{\bm{s}_i^2-\bm{x}_k^2}$.

In many cases it would be reasonable to expect the encounter rate or detection probability to depend on spatially-referenced covariates as well as distance. For example, the individual shown in Figure~\ref{fig:ER+detfun_habitat} is more inclined to go to a location in green habitat than a location in brown/orange habitat -- something that is reflected in Figure~\ref{fig:ER+detfun_habitat_encrate}, where we see that the individual's expected encounter rate with detector 3 is higher than with detector 2, despite the fact that detector 2 is closer to its activity centre (see Figure~\ref{fig:ER+detfun_ENwithdists}).

\subsubsection{Spatial effects at detectors}

If a detector is at a location that has particularly attractive habitat for the target species, one might expect that the encounter rate or detection probability at this detector might be higher than at detectors in less attractive habitat. For example, detector 3 has a higher encounter rate than detector 2 in Figure~\ref{fig:ER+detfun_habitat_encrate} because it is located in better habitat (see Figure~\ref{fig:ER+detfun_habitat}). This spatial effect can be incorporated at the trap level - by associating the habitat covariate ($z(\bm{x}_k)$ at detector $k$) with the detector, as in Table~\ref{tab:ERdetfun-detdata}. In this case, if the linear predictor without the habitat covariate at the detector was $\eta_{ijk}$ for individual $i$ on occasion $j$ at detector $k$, the effect of the covariate could be modelled using the linear predictor $\eta_{ijk}^*=\eta_{ijk} + \beta_z z_k$.

But this takes account only of $z$ at the detector location, not the location of the individual. When the individual's activity centre is at the detctor, both are equal to $z(\bm{x}_k)$, and considering an encounter rate or detection function model with an intercept that depends on $z(\bm{x}_k)$ is sensible. But even in this case, you might want to consider a model in which the intercept also depends on other values of $z$ in the vicinity of the activity centre. If animals are detected by moving past a detector, for example, more attractive $z$ values (e.g. better habitat for feeding) some distance away from their activity centres might result in them spending less time at the centre, and hence being less detectable at the centre than they might oterwise be. In the same way, attractive $z$ values at a distance from the activity centre would be inclined to make animals range farther and hence have greater $\sigma$. So you might want to consider models in which $z$ values at points in space other than the detector location, affect the intercept and/or the scale parameter of an encounter rate or detection function model. 

\subsubsection{Spatial effects away from detectors}

To account for the location of the detector \textit{and} the location of an individual's activity centre ($\bm{s}_i$ for individual $i$), we might want our linear predictor(s) to include  the difference between the habitat covariate value at detector $k$ ($z(\bm{s}_k)$) and the habitat covariate value at the activity centre $i$ ($z_i$): $\Delta_{ik}=\Delta(\bm{s}_i,\bm{x}_k)=z(\bm{x}_k)-z(\bm{s}_i)$. Including this in a linear predictor, we have $\eta_{ijk}^*=\eta_{ijk} + \beta_c \Delta_{ik}$, where $\eta_{ijk}$ is whateer the linear predictor for before the inclusion of $\Delta_{ik}$. This could be a linear predictor for the intercept and/or scale parameter. As an example, if we had a scale parameter without habitat covariates that was $\sigma=\exp{\beta_0}$ and then used $\Delta_{ik}$ above, we would have
\be
\sigma(\bm{s}_i,\bm{x}_k)&=&\exp\left\{\beta_0 + \beta_c[z(\bm{x}_k)-z(\bm{s}_i)]\right\}.
\label{eq:detcovs_sigma_ik}
\ee

Taking this line of thinking a step further, it is reasonable to expect that an individual's encounter rate at the location of a detector depends not only on the difference between the spatially referenced covariate at the detector and at the activity centre, but on all the other values of the spatially referenced covariate that are available to the individual. For example, an individual will have a lower expected encounter rate with the detector if there are many other more attracive $z$ values available to it than if there are no other more attractive $z$ values available to it.\todo[inline]{Insert something on habitat availability metrics here?}

\cite{Royle+al:13a} proposed a way of incorporating all $z$ values that are available to an individual, into expected encounter rate or detection probability models. This is to scale distance according to some habitat-dependent movement cost and then to use the least-cost distance instead of the straight-line distance between activity centre and detector in encounter rate and detection functions. There are other ways of using a habitat-dependent movement cost to define distance, which we touch on briefly after dealing with least-cost distances.

\subsection{least-cost paths and scaled distances}

\cite{Royle+al:13a} proposed that the least-cost path between two points $\bm{s}_k$ and $\bm{s}_i$ be used to measure the distance between them from an animal's perspective. This begs the question ``What is a least-cost path?''. Read on to find out.

Moving has a cost (energetically, at least), so that distance travelled (by individuals, or in the case of acoustic surveys, by sound) is a measure of travelling cost. The farther the movement, the greater the cost. When cost is measured only by distance, the least-cost path between any two points is just the shortest path between them (i.e. a straight line between them) and the least-cost distance is the straight-line distance between them (or great circle distance if the distance is big enough that curvature of the earth matters). So all SCR models involve using a least-cost path and a least-cost distance, albeit a particilarly simple one. \cite{Royle+al:13a} proposed scaling this distance in a particular way. Before looking at the detail of their scaling, we consider the issue of scaling distances in encounter rate models of detection function models in general, and how this relates to cost.

\subsubsection{Scaled distance}

The thing that multiplies $d$ in an encounter rate or detection function model can be interpreted as the cost per unit distance of travel. In models that use straight-line distance, it is $1/\sigma$ that multiplies $d$. If travelling is costly, $1/\sigma$ is large and individuals (or their sounds on an acoustic survey) are not detected far from their activity centres. If $1/\sigma$ is small, individuals (or their sounds) range far. In these models $\sigma^{-1}$ can be interpreted as the cost per unit distance, i.e. the rate at which cost accumulates per distance unit travelled.

Things get more difficult and more interesting when this cost per unit distance depends on where the individual is at the time -- when it depends on the habitat at each point in space, for example.

All implementations of SCR models to date involve discretising continuous space into $M$ separate cells, with the location of cell $m$ being $\bm{s}_m$. A path from $\bm{s}_i$ to $\bm{s}_k$ is defined by the ordered set of cells that are visited in moving from $\bm{s}_i$ to $\bm{s}_k$, and the cost of any path is the sum of the costs of the individual movements from one cell to the next in the path. Finding the path with lowest cost requires a cost rate function (giving the cost per unit distance moved) to be defined for every possible movement from one cell to a neighbouring cell, and a method for finding the path with the lowest possible sum of individual movement costs.

Given a cost rate function, there are efficient algorithms for finding least-cost paths (see next section). The cost rate function, however, is up to the user to define. With the above definition of cost, the cost of travelling between neighbouring cells $\bm{s}_m$ and $\bm{s}_{m^*}$ is  
\be
d^{(c)}(\bm{s}_m,\bm{s}_{m^*})&=&c(\bm{s}_m,\bm{s}_{m^*})d(\bm{s}_{m},\bm{s}_{m^*}),
\label{eq:detcovs_ecoldist}
\ee
\noindent
where $d(\bm{s}_{m},\bm{s}_{m^*})=||\bm{s}_{m}-\bm{s}_{m^*}||$ is the Euclidian (straight-line) distance between $\bm{s}_{m}$ and $\bm{s}_{m^*}$  and $c(\bm{s}_m,\bm{s}_{m^*})$ is the cost rate function that reflects the cost per unit distance of moving between $\bm{s}_m$ and $\bm{s}_{m^*}$. (In the context of least-cost paths, this is sometimes referred to as the \textit{resistance}, while its inverse $\sigma(\bm{s}_m,\bm{s}_{m^*})=c(\bm{s}_m,\bm{s}_{m^*})^{-1}$ would be referred to as the \textit{conductance}.) %Models that use straight-line distances have $c(\bm{s}_m,\bm{s}_{m^*})=\sigma^{-1}$.

\cite{Royle+al:13a} and \cite{Sutherland+al:15}\todo{And others?} used the following cost rate function:
\be
c(\bm{s}_m,\bm{s}_{m^*})&=&\frac{c^{(e)}(\bm{s}_m,\bm{s}_{m^*})}{\sigma_0}\;=\;\frac{\exp\left\{\beta_c\frac{z(\bm{s}_m)+z(\bm{s}_{m^*})}{2}\right\}}{\sigma_0},
\label{eq:detcovs_royle_cost_fn}
\ee
\noindent
where $z(\bm{s}_m)$ and $z(\bm{s}_{m^*})$ are the values of some habitat variable that is expected to affect the cost of travelling between neighbouring cells $\bm{s}_m$ and $\bm{s}_{m^*}$,  $\beta_c$ is a parameter that quantifies the strength of the effect, and $\sigma_0$ is the usual Euclidean distance scale parameter. They call $c^{(e)}(\bm{s}_m,\bm{s}_{m^*})d(\bm{s}_m,\bm{s}_{m^*})$ the ``ecological distance'', in which case $c^{(e)}(\bm{s}_m,\bm{s}_{m^*})$ can be interpreted as the ecological cost of travelling a unit distance. Note that when $\beta_c=0$ the ecological distance is equal to the Euclidean distance.

If we rewrite Eqn~\eqref{eq:detcovs_royle_cost_fn} in terms of the location-specific scale parameter, $\sigma(\bm{s}_m,\bm{s}_{m^*})$, we have 
\be
\sigma(\bm{s}_m,\bm{s}_{m^*})&=&\exp\left\{\beta_0-\beta_c\left[\frac{z(\bm{s}_m)+z(\bm{s}_{m^*})}{2}\right]\right\}.
\label{eq:detcovs_royle_sigma_fn}
\ee
\noindent
where $\sigma_0=\exp\{\beta_0\}$. Hence use of the ecological distance amounts to adding a location-specific covariate ($[z(\bm{s}_m)+z(\bm{s}_{m^*})]/2$ above) to the linear predictor of the scale parameter. This particular covariate is just one of any number of possible location-specific covariates, and will not be most appropriate for all applications. Notice, for example, that the assocaiated cost rate function is symmetric: the cost of moving from $\bm{s}_m$ to $\bm{s}_{m^*}$ is identical to the cost of moving from $\bm{s}_{m^*}$ and $\bm{s}_m$. If $z(\bm{s})$ was elevation, a cost rate function in which the cost of going uphill from $\bm{s}_m$ to $\bm{s}_{m^*}$ was greater than the cost of going downhill from $\bm{s}_{m^*}$ and $\bm{s}_m$ seems more appropriate. One might prefer a function similar to that below in this case:
\be
\sigma(\bm{s}_m,\bm{s}_{m^*})&=&\exp\left\{\beta_0-\beta_c[z(\bm{s}_m)-z(\bm{s}_{m^*})]\right\}.
\label{eq:detcovs_elev_sigma_fn}
\ee

Another potentially very useful option, suggested by \cite(secr:16)\todo{fix citation/bib file}, is to allow $\sigma$ to be density-dependent by using the covariate $[D(\bm{s}_m)+D(\bm{s}_{m^*})]/2$, where $D(\bm{s})$ is individual density at $\bm{s}$:
\be
\sigma(\bm{s}_m,\bm{s}_{m^*})&=&\exp\left\{\beta_0-\beta_c\left[\frac{D(\bm{s}_m)-D(\bm{s}_{m^*})}{2}\right]\right\}.
\label{eq:detcovs_elev_sigma_D_fn}
\ee

The is no one-size-fits-all cost rate function, and it is up to the user to define sensible cost rate functions for their problem, but the option to do so adds very substantially to the power of SCR analyses. You can select between cost rate functions using likelihood-based model selection criteria such as AIC or AIC$_c$, in the same way that you can select between encounter rate functions, detection functions, or density surface models. 

\subsubsection{Least-cost paths}

Efficient methods for finding least-cost paths when costs are not the same everywhere are not trivial, but fortunately the \texttt{R} libraries \texttt{igraph} and \texttt{gdistance} (which uses \texttt{igraph}) implement efficient methods to find least-cost paths, using Dijkstra's algorithm. Here we illustrate the idea, without giving details of the algorithm, which is due to \cite{Dijkstra:59}.

Figure~\ref{fig:detcovs-igraphplot} illustrates the implementation of a cost rate function on a simple 4$\times$4 cell discretisation of space. In the left plot, dark shaded cells have $z(\bm{s})=1$, light shaded cells have $z(\bm{s})=100$ and white cells are excluded (individuals can't move into them). The set of cells is represented as a network in the right plot of Figure~\ref{fig:detcovs-igraphplot}, with vertex number corresponding to cell number in the left plot, neighbouring cells are joined by lines, and the ecological distance between neighbours is shown on each line, using the cost rate function of Eqn~\eqref{eq:detcovs_royle_cost_fn} with $\beta_0=0$ and $\beta_c=1$. 

Dijkstra's algorithm is an efficient way of finding the least-cost path across networks like this. With this simple network it is easy to find the least-cost path between cells 16 and 2, for example, by inspection. It goes via cells 11, 8 and 3 and is of length $140+71+1.4+1=213.4$. This path is shown in Figure~\ref{fig:detcovs-lcpath}. In larger problems, finding the least-cost path by inspection in this way is infeasible, and so we use Dijkstra's algorithm to find it efficiently. (There are other efficient algorithms, but Dijkstra's is readily available to us in the \texttt{igraph} \texttt{R} library.)
\begin{figure}[ht]
\caption{\small A simple example of a 4$\times$4 cell discretisation of space (left) and corresponding network with ``ecological distances'' between cells (right). Cells are numbered 1 to 16; white cells are obstacles, light gray cells have habitat that facilitates movement, dark gray cells have habitat that inhibits movement. See text for deails.}
\centering
\includegraphics[width=12cm]{keepfigure/igraphplot.pdf}
\label{fig:detcovs-igraphplot}
\end{figure}

\begin{figure}[ht]
\caption{\small The least-cost path between cells 16 and 2, calculated using the distances shown in the right plot of Figure~\ref{fig:detcovs-igraphplot}.}
\centering
\includegraphics[width=6cm]{keepfigure/lcpath.pdf}
\label{fig:detcovs-lcpath}
\end{figure}


\subsection{Other cost-based distance measures}

\todo[inline]{Average cost distance; Markov model stationary distribution idea?}


\subsection{Summary}

The idea of using spatial covariates to scale distance differently at different points in space, or of ``ecological distance'' instead of straight-line distance to measeure the distance between points in space from the target species' perspective, is a powerful one. It facilitates a number of useful ecological interpretations, including inferences about landscape connectivity, it better reflects individuals' behaviour in heterogeneous environments, and \cite{Sutherland+al:15} showed that using Euclidian distance in highly structured environments when individuals move according to an ecological, non-Euclidian, distance, results in negatively biased density and abundance estimates. 

\textit{Mention least-cost distance in acoustic surveys more? Would be nice to have an application!}

\subsection{Example analyses}

\todo[inline]{David's initial thoughts:

(0) Some examples of how to use gdistance and igraph in SCR context might be very useful!

(1) Need to make point that you get negative bias if there is ecological cost in highly structured habitat and you ignore it. Also illustrate landscape connectivity. 

(2) Would be nice to provide some user-friendly distance functions in package scrmlebook, and to illustrate their use.

(3) Candidate datasets: 
(a) Belize jaguars, (b) Koustubh Sharma's snow leopards, (c) something with altitude as z, (d) one of Andy's or Chris' datasets, maybe with different cost rate function. Don't have permission to use any of these yet, but don't anticipate any problem gettng it. I have been doing some snow leopard SCR analyses with Koustubh Sharma: some cool plots from these analyses are shown below. Point estimates of abundance are 24 (16, 39) when use Euclidian distance, and 28 (17, 50) when you use non-Euclidian - so point estimates are consistent with hypothesis of negative bias that you get negative bias by using Euclidian distance in highly structured habitats (although numbers are very small and CIs big!).}



\begin{figure}[ht]
\caption{\small Plot of ruggedness covariate in Tost snow leopard survey region, together with locations of numbered camera traps.}
\centering
\includegraphics[width=12cm]{keepfigure/cameras.pdf}
\label{fig:detcovs-Tost-SLcams}
\end{figure}


\begin{figure}[ht]
\caption{\small Estimated log(snow leopard density) when density and the distance cost function have ruggedness in the linar predictor.}
\centering
\includegraphics[width=12cm]{keepfigure/TostDhat.pdf}
\label{fig:detcovs-Tost-TostDhat}
\end{figure}

\begin{figure}[ht]
\caption{\small Plots of theestimated probability of being captured by each of four cameras (back dots in white circles), as a function of activity centre location and terrain ruggedness. Note how the probabilities stay higher in regions of high ruggedness (refer to Figure~\ref{fig:detcovs-Tost-SLcams}), which is the habitat that the SCR analysis estimates to be preferred by snow leopards.}
\centering
\includegraphics[width=12cm]{keepfigure/spatialDetProbs.pdf}
\label{fig:detcovs-Tost-Detprobs}
\end{figure}


\begin{figure}[ht]
\caption{\small Plot of least-cost paths between a few selected points (from green dots to red dots), as estimated by the SCR analysis. The paths are superimposed on a map of terrain ruggedness. Note how they tend to follow the more rugged regions.}
\centering
\includegraphics[width=12cm]{keepfigure/someLCpaths.pdf}
\label{fig:detcovs-Tost-someLCpaths}
\end{figure}
