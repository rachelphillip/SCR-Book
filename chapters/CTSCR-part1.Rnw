\chapter{Continuous-time SCR models}
\label{chap:continuous-time}

\abstract{This chapter introduces models that use the observed times of capture and hence remove the need to impose an occasion structure on data collected in continouous time.  }

\section{Introduction}
\label{sec:CT intro}
\index{}

Spatial and non-spatial application of classic CR typically involves physically capturing and marking individuals at discrete sampling occasions in time, and the survey process generates well-defined sampling occasions. But as discussed in Chapter~\ref{chap:det-types}, these days it is possible to use non-traditional detectors that do not involve any physical capture and release (hence the term \textit{passive} detectors), in which case there are no well-defined sampling occasions.

Researchers using passive detectors typically aggregate their data into sampling occasions so that the data are then suitable for analysis with traditional models. However devices like camera traps are able to record the actual times of capture and aggregating time-to-detection data into discrete capture occasions imposes an artificial construction on the data for reasons of analytic convenience. It is always a better policy to find a statistical formulation that fits the data than to shoehorn the data into a form that fits statistical formulations that happen to be available. Furthermore, aggregating data into capture occasions introduces subjectivity (the occasion length chosen by the analyst) and discards some information (the exact times of capture). 

As discussed previously, SCR methods assume independence between traps, i.e.\ the probability of being caught at a particular trap is not affected by a capture elsewhere. In general, the only benefit to imposing the artificial construct of occasions on continuous data is to remove correlation in the capture times so that the independence assumption is not violated. There is also likely to be little additional information in multiple captures in a short time \citep{Royle09, Royle09b}.

However, binning the data into occasions does not allow the inclusion of continuous covariates (for example weather) and limits the potential inferences that can be made regarding underlying biological processes that occur in continuous-time (CT).  Furthermore, the use of occasions often leads to the assumption of equal occasion lengths so that the interpretation of the capture probabilities are comparable across different periods. Such an assumption is questionable when factors like trap failure exist. It is common to aggregate the data into 24 hour periods \citep{Maffei04, Foster12} which can lead to the so called ``midnight problem'' \citep{Jordan11} where an animal photographed either side of midnight counts as two encounters as opposed to only counting as one encounter when captured twice during the same day. If animals are most active around the cutoff then capture probability will be positively biased, and this is likely to be true for nocturnal cats. Capture histories that use daily occasions for low density animals also tend to have many zeros \citep{Foster12}.

The CT hazard results in a model in which variation in detectability is modelled at a finer resolution than DT models with occasions.\todo{need to find best place for this statement} 

\todo[inline]{link potential benefit of continuous covariates back to chapter 4's discussion on behavioural effects that disappear.}

\section{Modelling time-to-detection data}
\label{sec: modelling CT data}
\index{}

The CT framework involves modelling the process generating detections as a temporal Non-Homogeneous Poisson Process (NHPP) in which the events are detections. This approach assumes that the encounter rate for the $i$th individual at the $j$th detector at any time is independent of the individual's capture history up to that time. Note that the model is easily extended to be of type $M_b$ by estimating different detection levels before and after first capture.

In a CT formulation, the discrete-occasion expression for the probability of detecting individual $i$ at detector $j$ during occasion $k$ is replaced by an encounter rate function in the same way as seen in Chapter~\ref{chap:ERdetfun}. The difference is that the traditional SCR models make an implicit assumption that the expected encounter rate is constant through time but here the encounter rate function is extended to depend on time in addition to distance. Consequently the expected encounter rate for the $i$th individual and the $j$th trap at time $t$ now depends on both space (in terms of the distance from the trap to the activity centre $d_{ij}$) and time and is denoted as $\lambda(d_{ij},t)$.

Note that the survival analysis literature tends to refer to a time-dependent encounter rate as a ``hazard''. While these two terms can be used interchangeably the convention followed here is to use the term ``hazard'' when specifically referring to the time-dependent component of the encounter rate function, with the hazard in this case being
the hazard of detection rather than hazard of death.

The probability of individual $i$ not being detected by trap $j$ over the whole survey $T$ (also called the ``survivor function'') is 

\[ S_j(T, d_{ij})=\exp{\left(-\int_0^T \lambda_j(d_{ij},t)\;dt\right) }\] 

and hence $1 - S_j(T, d_{ij})$ is the probability of detection in $(0,T)$.

Similarly the combined expected encounter rate over all traps at time $t$ is $\lambda_\cdot(t,\bm{d}_i)=\sum_{j=1}^J \lambda_j(d_{ij},t)$ where the vector $\bm{d}_i=(d_{i1},\ldots,d_{iJ})$ represents distances to all detectors. The overall probability of detection in $(0,T)$ over all detectors is therefore $p_\cdot(\bm{d}_i)=1-S_\cdot(T, \bm{d}_i)$, where $S_\cdot(T,\bm{d}_i)=\exp{\left(-\int_0^T \lambda_\cdot(\bm{d}_i,t))\;dt\right)}$ is the overall survivor function. 

By considering the vector of capture times for the $i$th individual at the $j$th detector $\bm{t}_{ij}$ to be a realisation of a NHPP in time, an expression for the probability density function (pdf) of $\bm{t}_{ij}$ is obtained. This is analogous to the probability of obtaining a capture history at detector $j$ for individual $i$ in a discrete-time formulation, but it is worth noting that both the length of $\bm{t}_{ij}$ (denoted as $\omega_{ij}$ \todo{I'm a little unsure about the notation because $\omega$ is also used to represent capture histories?}) and the times that it contains are random variables, whereas for a discrete-occasion survey the length of a capture history is fixed by the number of occasions used.

Using a result from Poisson process theory \cite[see][Theorem 2.1 on page 30, and Equation (2.13) on page 32]{Cook07} \todo{is the citation necessary?}, and assuming that, conditional on the activity centre location, the times of detections are independent, the detection times for individual $i$ at detector $j$ can be modelled as a NHPP with pdf:

\begin{align}
f_j(\bm{t}_{ij} \mid d_{ij})
&= S_j(T,d_{ij})\prod_{r=1}^{\omega_{ij}} \lambda_j(d_{ij},t_{ikr})
\label{eqn:f_j(t)}
\end{align}

A heuristic explanation of this expression for a hypothetical individual that is captured at a particular trap twice is given in  Figure~\ref{fig:Times1}. It is necessary to first model the probability that the individual is not captured from the start of the survey until the time of first capture, and this probability is found by $\exp{-\int_0^{t_1} \lambda_j(d_{ij},t) \; dt}$. The individual is then captured at the appropriate encounter rate $\lambda_j(d_{ij},t_1)$. Next the probability of no capture from $t_1$ until $t_2$ is found in the same way and the second capture occurs at the rate of $\lambda_j(d_{ij},t_2)$. Lastly, it is necessary to model the fact that the individual evades capture from $t_2$ until the end of the survey at time $T$. 

\begin{figure}[ht]
\caption{\small The plot depicts a time-dependent hazard (for a given distance) and shows how two captures (at times $t_1$ and $t_2$) are modelled when they are treated as a NHPP. Multiplying these terms together leads to the expression $e^{-\int_0^T \lambda_j(d_{ij},t)\; dt} \prod_r \lambda_j(d_{ij},t_{ijr})$.}
\centering
\includegraphics[width=12cm]{keepfigure/ModellingTimes.pdf}
\label{fig:Times1}
\end{figure}

Equation~\ref{eqn:f_j(t)} can be fitted into the alternative formulation presented in Section~\ref{sec:det-types-blocks}. The marginal distribution of the detection frequency at detector $j$ for the $i$th animal ($\omega_{ij}$) has a Poisson distribution with parameter $\Lambda_j(d_{ij})=\int_0^T \lambda_j(d_{ij},t)\;dt$, and the conditional pdf of detection times $\bm{t}_{ij}$, given $\omega_{ij}$ is

\begin{align}
f(\bm{t}_{ij} \mid \omega_{ij},d_{ij})
&=\omega_{ij}!\prod_{r=1}^{\omega_{ij}}\frac{\lambda_j(d_{ij}, t_{ijr})}{\Lambda_j(d_{ij})}.
\label{eq:f_j(t|omega)}
\end{align}

\todo[inline]{Not sure if I should show the full LL - the reason I was considering this is to discuss sufficiency and show that in a general sense at least there is information in the times. Just seems a bit too technical relative to the rest of the book.}

\section{Parameterising the expected encounter rate function}
\label{sec:CT hazards}
\index{}

When the encounter rate function varies through time, the expected encounter rate of individual $i$ being detected at trap $j$ depends as before on the distance between trap $j$ and individual $i$'s activity centre, but also depends on time. This scenario is more realistic as most animals are more likely to be caught at certain times rather than others depending on their behaviour and activity patterns. The $_{ij}$ notation is suppressed in this section for brevity.   

The time-dependent detection hazard is conceived here as a cyclical hazard that repeats over time after a specified cycle length. This approach facilitates the modeling of a daily pattern to detectability that, for example, can result in nocturnal animals being very unlikely to be captured during the day. Using a cyclical hazard means that the hazard is constrained to begin and end at the same point each cycle. 

\subsection{Seperating time and distance}
\label{subsec:independent}

One way to parameterise the expected encounter rate function is to have a component that depends on distance ($\lambda(d)$) and a component that depends on time ($h(t)$). This parameterisation effectively means that these two components do not interact and hence the effect of time is independent of the effect of distance (and vica versa). 

\begin{equation}
\lambda(d,t) = \lambda(d) \times h(t) 
\label{eqn:hazt1}
\end{equation}

The second component $h(t)$ determines the shape of the detection hazard over time and the resulting time-dependent hazard of detection is scaled by $\lambda(d)$ that depends on the distance from the individuals' activity centre to the detector. 

%This scaling factor depends on the parameters that appear in the usual detection function, i.e.\ for the half-normal form the parameters in $\bm{\theta}$ include detectability ($g_0$) and dispersion characteristics ($\sigma$). 

A flexible way to model $h(t)$ is to use regression splines. Regression splines are a form of less rigid parametric fitting that can be thought of as fitting piecewise polynomials to segments of the data. A regression spline takes the form of a weighted sum of smooth functions of the data, and these functions of the data are known as basis functions. The flexibility of regression splines increases with the degrees of freedom or knots, i.e.\ increasing the knots increases the number of parameters that need to be estimated and increases the function's flexibility. 

An array of different types of basis functions are available that produce splines with different features. Cyclic cubic regression splines are used here to produce hazard cycles of a specified length that repeat over time.

The $h(t)$ component with a flexible shape is therefore parameterised as follows:

\begin{equation}
h(t) = e^{f(t)}
\label{eqn:spline}
\end{equation}
 
where $f(t) = \beta_1 b_1(t) + \beta_2 b_2(t) + \ldots + \beta_p b_p(t) $, the $b_p(t)$ refer to $p$ basis functions, $p$ is determined by the specified degrees of freedom (denoted as $K$), and the log link ensures a positive detection hazard. 

The R package \emph{mgcv} \citep{Wood14} is used to construct the necessary basis functions that are then used as ordinary covariates in the hazard model. Note that the intercept of the spline coefficients is redundant because its role is effectively subsumed by the $\lambda(d)$ scaling factor and hence it is ommitted from Equation \ref{eqn:spline} above. Furthermore, a degree of freedom is lost when specifying hazards with a cyclic cubic spline form. The number of degrees of freedom ($K$) for different spline hazards that are reported in this chapter matches the number passed to the \emph{gam} function in package \emph{mgcv} and hence the actual number of parameters estimated in the reported models is two less than $K$.

Figure \ref{fig:GAMhaz} depicts three fictitious examples of different possible hazard shapes that can be modelled with regression splines. These plots focus on the shape of the detection hazard through time and hence are scaled so that they integrate to 1. The top panel shows a simple detection hazard that rises to a peak about halfway through the cycle and then falls away again. The second example is similar but remains around its peak for a longer time. The final example shows a hazard that has two peaks, if one considers the 24 hour hazard cycle to be in sync with a 24 hour daily cycle then these peaks occur roughly at dawn and dusk. These shapes are produced by making up a series of data points that follow a certain pattern, extracting the basis functions, and then using the basis functions as covariates for fitting a generalised linear model with a log link. 
For example the fitted model for the hazard shape with K = 4 that is shown in the top panel is $h(t) = e^{1.145 b_1(t) + 1.284 b_2(t)}$. 

\begin{figure}[ht]
\caption{\small Three examples of possible detection hazard shapes that can be modelled using regression splines of increasing complexity. From top to bottom the degrees of freedom (as passed to the \emph{gam} function in package \emph{mgcv}) are 4, 8 and 10.}
\centering
\includegraphics[width=12cm]{keepfigure/GAMhaz.pdf}
\label{fig:GAMhaz}
\end{figure}

\todo[inline]{something about ancillarity and density? Again not sure if too technical. Could maybe leave out the maths and just say something like what appears below:}

A consequence of seperating the effects of time and distance is that there is no information in the capture times about density. Therefore a discrete time model estimator that ignores the capture times will not be biased for density (providing that the underlying data is being generated from such a process).  

\subsection{A more general formulation}
\label{subsec:generalform}

The expected encounter rate function can be specified so that the effects of time and distance interact. This means that the effect of time on the expected encounter rate depends on the distance between the trap and an individual's activity centre (and vica versa). One way to achieve this is to make the $\sigma$ parameter depend on time. This leads to a model where the extent that an animal ranges depends on the time of day and so has a useful biological interpretation.

\begin{equation}
\lambda(d,t) = \lambda_0\exp\{-d^2/(2\sigma(t)^2)\}
\label{eqn:hazt2}
\end{equation}
where $\sigma(t)=\exp(\beta_0 + \sum_{k=1}^K\beta_k\times b_k(t))$

Figure \ref{fig:3DHaz1} depicts how such an encounter rate function depends on both time and distance. If the detector is very far away from an animal's activity centre, the animal will hardly be detectable, no matter what time of day it is. Similarly, it is apparent that at times of the day when an animal does not range far (and so has a low value for $\sigma$) the expected encounter rate falls away much more quickly with distance than at times that correspond to higher higher values for $\sigma$.  

Note that with this parameterisation the $\lambda_0$ parameter does not depend on time. Effectively this means that if a detector is placed right on top of an animal's activity centre the expected encounter rate will be constant through time, which explains why the function tends to a flat surface as distance tends to zero. An encounter rate function with this characteristic may be plausible for certain types of animals, for example if an animal tends to rest very close to its activity centre and even during resting it moves around a little and is hence still detectable \todo[inline]{not sure about this last statement?}. Of course it is possible to extend the flexibility of the function and allow $\lambda_0$ to also depend on time, or to explore an alternative sort of parameterisation similar to \citet{Efford14}.

\begin{figure}[ht]
\caption{\small The three dimensional figure depicts a hypothetical expected encounter rate function that is parameterised according to Equation \ref{eqn:hazt2}. It is apparent how the expected encounter rate through time depends on distance and vica versa. The plot shows one cycle of 24 hours. }
\centering
\includegraphics[width=12cm]{keepfigure/3DHazplot.pdf}
\label{fig:3DHaz1}
\end{figure}

\todo{A paragraph and perhaps a table that summarise simulation results that hopefully show that the DT estimator is biased if the underlying process is akin to this sort of hazard and that in such a case there is important information in the capture times.}

\subsection{Linking with the detection function}
In Chapter~\ref{chap:ERdetfun}, Equation~\ref{eq:ER+detfun.binaryp} is used to link the detection function with the encounter rate model. However in that case an implicit assumption made is that the encounter rate is constant with time (which is why $\int_0^T \lambda(d_{ij}) \; dt = \lambda(d_{ij}) T$). 

Here the same approach is used to link the time-dependent encounter rate function with the detection function but it is generalised for time-dependent hazards. If $T_k = t_k-t_{k-1}$ where $t_{k-1}$ is the start of occasion $k$ and $t_k$ the end, then the linkage is accomplished by specifying the detection probability for detector $j$ in occasion $k$ of duration $T_k$ in terms of the encounter rate function as follows. 

\begin{equation}
p_k(d)=1-\exp{\left(-\int_{t_{k-1}}^{t_k} \lambda_{j}(d,u) \;du \right)} 
\label{eqn:hazlink}
\end{equation} 

The paramaterisation used in Section~\ref{subsec:independent} means that $\int \lambda_j(d,t) \; dt = \lambda_j(d) \int h(t) \; dt$ and hence

\begin{equation}
\lambda_k(d) = \frac{-log(1-p_{jk}(d))}{ \int_{T_k} h(t)\;dt} 
\label{eqn:hazlink2}
\end{equation}

The more general formulation (Section~\ref{subsec:generalform}) does not allow this simplification and hence in that case:

\begin{equation}
\int \lambda_k(d,t) \; dt = -log(1-p_{jk}(d)) 
\label{eqn:hazlink2}
\end{equation}

There are therefore many different encounter rate functional forms that can lead to the same detection function.

\section{Accounting for varying effort}
CT models readily accommodate detectors that are operational for different periods of time by setting the expected encounter rate at a detector to zero while the detector is out of operation (i.e.\ $\lambda_j(d,t)=0$ if detector $j$ was not operating at time $t$). 

This is the basis of the varying effort model for binary detectors and counts in \citet*{Efford13b}. However it should be noted that in that case it was assumed that the effect on the hazard is proportional to a standard unit of effort, i.e.\ if the effort for a given occasion is twice that of a standard unit of effort then the detection hazard gets doubled. The varying effort model can therefore accommodate occasions that are of different lengths but in order to do this it implicitly assumes that the hazard is constant through time. The assumption of a constant encounter rate through time is also made when the expected encounter rate for any duration $T$ is scaled in proportion to $T$, and also for the discussion on effort in Section \ref{sec:ERdetfund-effort}.

The CT framework generalises this idea and can for example recognise that a detector failing for a period of time during the day may have little effect on the detectability of a nocturnal species. The ability of the CT model to properly handle varying trap exposure is what leads to the construction of the single-catch trap likelihood seen in Section \ref{sec: CT SC}. It is more difficult to accommodate detectors when their time of failing is unknown, and that problem is not addressed here.  

\todo[inline]{Can put in a little more maths to show how shape of det fn is no longer normal for different durations? Seems a bit technical which is why I have left this out.}

\section{Learning about animal activity patterns}
\label{sec: activity}
\index{}

Understanding the activity patterns of animals can be used to learn about animal behaviour. \citet{Cowan12} use single-catch traps with timing devices to understand the behavioural response of possums that are caught, and \citet{Harmsen09} make inference about the hourly activity of jaguars and pumas from the frequency distribution of hourly capture times (397 for jaguars and 413 for pumas). However using a histogram with $X$ bars (a) requires arbitrary decisions about where the intervals start and end for each bar, (b) involves estimating ($X-1$) parameters, (c) does not give a smooth estimate of activity pattern change over time, and (d) does not include a rigorous framework for quantifying the associated uncertainty or selecting between alternative models. Using a spline involves no arbitrary decisions about intervals, involves estimating as many or as few parameters as the data support, provides a much more realistic smooth estimate of activity pattern change and a rigorous framework for model selection, uncertainty quantification, and testing hypotheses about activity patterns. 

Simulations confirm that CT models are able to estimate the underlying detection hazard cycle well. As one would expect, the variability in these estimates is higher for more complicated hazards that require more parameters, and decreases with more data. 

\citet{Ridout09} quantify the overlap in activity patterns between different species from camera trap data. They use kernel density estimation for circular data to estimate the continuous activity pattern distributions, and then look at different measures of overlap. The measures of overlap used in \citet{Ridout09} could also be applied to the estimated activity patterns from the estimated spline hazards. 

\section{Modelling heterogeneity in detection}
\label{sec: CT hetero}
\index{}

Up to this point all the plots have used cyclical hazards with a cycle duration of 24 hours with the idea of mapping activity patterns on to a daily 24 hour cycle. However it is possible that detectability can also depend on cycles with a longer duration. For example a fuller moon leads to brighter hunting conditions and \citet{Harmsen11} report that the two main prey species of jaguars and pumas both exhibited reduced activity under these conditions, and discuss different potential predator strategies in response to this reduction in activity. Figure \ref{fig:lunarhaz} shows a detection hazard with a 30 day cycle that could be used to represent a longer process such as the lunar cycle. Note however that it is likely that if such a process existed it would be on top of a shorter daily detection cycle and  both would probably need to be modelled together. 

The vertical red lines in Figure \ref{fig:lunarhaz} mark the 24 hour occasions that would be typically used in a DT model, and it is evident that a hazard such as this would lead to heterogeneity in detection across occasions. When detectability varies through time there are several different approaches that can be taken with a traditional DT model. The variation across occasions can be ignored and a single detection function fitted to all occasions, which may not lead to biased estimates of density but will limit the inferences that can be drawn from the estimated detection function. At the other extreme, a DT model can estimate separate detection function parameters for every occasion but that approach can lead to a large increase in the number of parameters and an associated increase in variance. An intermediate approach would be to reduce the number of parameters by modelling a parametric trend across time at the resolution of an occasion. A CT model can effectively achieve the same thing without the need for occasions: assuming a constant hazard corresponds to homogeneity in detection while using a flexible spline allows a suitable level of temporal heterogeneity in detection to be modelled. However, the CT approach provides a flexible modelling framework that allows the data to inform how many parameters are required to model detectability at a finer temporal resolution, and hence allows heterogeneity in detection to be modelled in a more natural, parsimonious and flexible way compared to traditional DT models. 

\begin{figure}[ht]
\caption{\small A detection hazard with a 30 day (720 hours) cycle. The vertical red lines demarcate the 24 hour occasions to show the variation in detectability between occasions. } 
\centering
\includegraphics[width=\textwidth]{keepfigure/LunarHaz.pdf}
\label{fig:lunarhaz}
\end{figure}

\section{Example analyses}
\label{sec: CT Prox Examples}
\index{}

The fitting of CT SCR models to data from passive detectors is illustrated in this section using two different datasets. The first application involves a real dataset from a camera trap survey of jaguars that took place in the Cockscomb Basin Wildlife Sanctuary in Belize, and the second involves simulated data that include two different sessions. The objective in the first case is to explore the different ways of parameterising the expected encounter rate function, and in the second to look at how model selection tools can be used to explore different hypotheses. 

\subsection{Application to a camera trap survey on jaguars}
\label{subsec:CTJagsApp}

The function \texttt{CT.SCR} is used to fit the CT models. As before the necessary data and associated fitted objects are obtained by loading the library \texttt{scrmlebook} package.

{\small
<<CT-Jags-load, include = TRUE, echo = TRUE, eval=TRUE>>=
library(scrmlebook)
data("Jags-data")
setwd("~/Git/SCR-Book/scrmlebook/data")
load("Jags-fits.RData")
@
}

The first object contains the CT capture history from this survey along with the traps object, the mask object (created with the \texttt{secr} library) and starting values. You will notice that instead of capture occasions there are actual times of capture. The second command loads all the models that are fitted in this section. Details of the data and the fits can be viewed in the scrmlebook help files as follows:

{\small
<<CT-jags-load-help, include = TRUE, echo = TRUE, eval=FALSE>>=
help("Jags-data")
help("Jags-fits")
@
}

The main function that fits the CT models is called \texttt{CT.SCR}. It is up to the user to specify how flexible the time-dependent component of the encounter rate function should be, and also to choose how the encounter rate function is parameterised. Lets start with the form given by Equation \ref{subsec:independent} where the effect of time and the effect of distance are independent. 

The time component $h(t)$ of the function is modelled with a cyclic cubic spline with a cycle duration of 24 hours, and models are fitted with three different values for K (4, 6 and 8). Note that these values for K are the values that are passed to the \texttt{mgcv} R package and not the number of spline parameters that are actually estimated. For the independent parameterisation the number of spline parameters estimated will be 2 less than the value for K (the intercept is redundant and one degree of freedom is lost when specifying a cyclic cubic spline). For example the code below fits a model with a cyclic cubic spline detection hazard with K = 4, and where the effects of time and distance are independent:

\begin{verbatim}
IndHaz.K4  <- try(nlm(f=CT.SCR, p=start.K4, data_ = JagsCaptures, 
trap.objs_ = Traps, mask.files_ = Mask, haz.type_= 'IndU', 
haz.k_ = c(4), endpoint_ = 2160, mesh.size_ = 20, hessian=T, trace=FALSE))
\end{verbatim}

%IndHaz.K6  <- try(nlm(f=CT.SCR, p=start.K6, data_ = JagsCaptures, 
%trap.objs_ = Traps, mask.files_ = Mask, haz.type_= 'IndU', haz.k_ = c(6), 
%endpoint_ = 2160, mesh.size_ = 20, hessian=T, trace=FALSE))

%IndHaz.K8  <- try(nlm(f=CT.SCR, p=start.K8, data_ = JagsCaptures, 
%trap.objs_ = Traps, mask.files_ = Mask, haz.type_= 'IndU', haz.k_ = c(8), 
%endpoint_ = 2160, mesh.size_ = 20, hessian=T, trace=FALSE))

The code firstly calls \texttt{nlm} which is an optimiser to maximise the likelihood (well actually it is a minimiser so it rather minimises the negative log likelihood). The arguments that are supplied include starting values, the data frame, an SCR traps object, an SCR mask object, the chosen parameterisation for the encounter rate function, the value for K (as passed to \texttt{mgcv}), the study duration (via the \texttt{endpoint\_} argument), and the mesh size which is used to determine how fine the grid for the numerical integration in the time dimension is. Lastly \texttt{hessian = T} asks for the estimated hessian matrix that is used to get the variance-covariance matrix for the parameter estimates.

The parameter estimates are found in the \texttt{\$estimate} attribute of the model objects. The order of parameters is: density, $\lambda_0$, $\sigma$, and then the spline parameters. Remember that the density estimates need to be backtransformed for interpretation. The density estimates (per 100 km$^2$) are shown below: 

<<DensEsts1, eval = TRUE>>=
exp(Jags.IndHaz.K4$estimate[1])*10000 ; 
exp(Jags.IndHaz.K6$estimate[1])*10000 ; 
exp(Jags.IndHaz.K8$estimate[1])*10000 
@

It is apparent that there is virtually no difference in the density estimates. Recall that the specification of the encounter rate function used here seperates the effects of time and distance and a consequence of this is that the capture times do not have any information about density. It is therefore not surprising that the different models for capture times do not lead to different estimates of density.

The function \texttt{CalcAICc} calculates the AICc's from the different CT models and the best fitting detection hazard as selected by AICc is the one with K = 6 (and hence has 4 spline parameters):

<<AICs1, eval = TRUE>>=
Calc.AICc <- function(mod.objs_, decimals_ = 2, n.inds_){
  if (is.list(mod.objs_)){
    n.mods <- length(mod.objs_)
    output <- NULL
    for (i in 1:n.mods){
      K = length(mod.objs_[[i]]$estimate)
      AIC <- 2 * mod.objs_[[i]]$minimum + 2*K
      AICc <- AIC + (2*K*(K+1))/(n.inds_ - K - 1)
      output[i] <- round(AICc, decimals_)
    }
    return(output)
  } else {print("Model objects must be a list")}
}

aics <- Calc.AICc(mod.objs_=list(Jags.IndHaz.K4, Jags.IndHaz.K6, Jags.IndHaz.K8),
                  decimals_ = 2, n.inds_ = 17)
print(xtable(aics, digits=2, 
             caption="AICc's from independent hazard models.",
             label="CTAICc1"))
@

Figure \ref{fig:indhaz1} shows the estimated detection hazards for the three different models. You can see how the estimated hazard shape changes as K increases. The model that uses a hazard with K = 6 has the lowest AIC and hence a hazard with a crepuscular (active during twilight) shape is deemed most appropriate for modelling detection of jaguars in the Cockscomb Basin Wildlife Sanctuary in Belize. 

\begin{figure}[ht]
\caption{\small Three estimated detection hazards ($h(t)$) fitted to the jaguar data using the independent parameterisation. } 
\centering
\includegraphics[width=\textwidth]{keepfigure/IndHaz1.pdf}
\label{fig:indhaz1}
\end{figure}

The detection hazards in Figure \ref{fig:indhaz1} show the scaled hazards for a 24 hour hazard cycle i.e.\ there is no effect of distance and the focus is really on the shape of the hazard function (the $h(t)$ component). Curves can also be plotted for a given distance that show the expected encounter rate for any duration of time. Figure \ref{fig:indhaz2} shows such a plot for two distances over a time duration of 48 hours. The `Prob' value shown in the legend is the probability of being caught at least once over that duration of time. 

\begin{figure}[ht]
\caption{\small Expected encounter rates fitted to the jaguar data using the independent parameterisation. Curves are shown for two different distances. } 
\centering
\includegraphics[width=\textwidth]{keepfigure/IndHaz2.pdf}
\label{fig:indhaz2}
\end{figure}

Now a new set of models that use the encounter rate function specification given in Equation \ref{subsec:generalform} are fitted. Note that these models take a long time to fit and that the fitted model objects can be found in the \texttt{scrmlebook} library. The code to fit a model with K = 4 and with the dependent encounter rate function form is as follows:

\begin{verbatim}
DepHaz.K4  <- try(nlm(f=CT.SCR, p=start.K4, data_ = JagsCaptures, 
trap.objs_ = Traps, mask.files_ = Mask, haz.type_= 'Dep', 
haz.k_ = c(4), endpoint_ = 2160, mesh.size_ = 20, hessian=T, trace=FALSE))
\end{verbatim}

%DepHaz.K6  <- try(nlm(f=CT.SCR, p=start.K6, data_ = JagsCaptures, 
%trap.objs_ = Traps, mask.files_ = Mask, haz.type_= 'Dep', haz.k_ = c(6), 
%endpoint_ = 2160, mesh.size_ = 20, hessian=T, trace=FALSE))

%DepHaz.K8  <- try(nlm(f=CT.SCR, p=start.K8, data_ = JagsCaptures,
%trap.objs_ = Traps, mask.files_ = Mask, haz.type_= 'Dep', haz.k_ = c(8), 
%endpoint_ = 2160, mesh.size_ = 20, hessian=T, trace=FALSE))

The density estimates are shown below. The estimates from the model with K = 4 are now slightly different to the other two models.

<<DensEsts2, eval = TRUE>>=
exp(DepHaz.K4$estimate[1])*10000 ; 
exp(DepHaz.K6$estimate[1])*10000 ; 
exp(DepHaz.K8$estimate[1])*10000 
@

Visualising the estimated encounter rate function is a little more complicated because of the interactive effect of time and distance on the expected encounter rate. If we want to view a 2-dimensional plot of the effect of time (or distance), we need to do this for a specified value of distance (or time). Figure \ref{fig:dephaz1} depicts these two-dimensional plots from the model with K = 6, and Figure \ref{fig:dephaz2} shows a 3-dimensional plot of the expected encounter rate.

\begin{figure}[ht]
\caption{\small Expected encounter rates fitted to the jaguar data using the dependent parameterisation. The left hand plot shows the effect of time (for given distances) and the right hand plot the effect of distance (for given times).} 
\centering
\includegraphics[width=\textwidth]{keepfigure/DepHaz1.pdf}
\label{fig:dephaz1}
\end{figure}

\begin{figure}[ht]
\caption{\small The time and distance dependent expected encounter rate function fitted to the jaguar data and shown in 3 dimensions. } 
\centering
\includegraphics[width=\textwidth]{keepfigure/DepHaz2.pdf}
\label{fig:dephaz2}
\end{figure}

\subsection{Application to simulated data from two sessions}
\label{subsec:CTSimApp}

This example uses a fictitious dataset that is based on the Belize jaguar camera trap survey data. Lets pretend that a survey was run over two different time periods, one corresponding to the dry season and another to the wet season. In both seasons a regular array of 20 paired camera stations was used for 90 days. The wet season is cooler than the dry season and so it is feasible that there could be a difference in the activity patterns of jaguars during the different seasons. Being able to estimate detection hazards within a formal modelling framework facilitates the use of standard model selection tools to explore the support in the data for competing hypotheses.

The dataset called `Sim.data.seasons` contains the data from the two surveys. You will notice that the data from the different seasons are used as different sessions. Models are fitted to these data using the independent parameterisation given in Equation \ref{subsec:independent}. 

A common detection hazard is first fitted for both seasons (sessions) followed by a model that uses season-specific hazards. Splines with K = 6 are used and for simplicity the same value of K is used in both seasons. There are model objects corresponding to other values for K in `Jags-fits.RData'. The AICc's from these models are shown below followed by a plot of the estimated encounter rate functions:

<<AICs2, eval = TRUE>>=
aics2 <-  Calc.AICc(list(IndHaz.K6.common, IndHaz.K6.sep), 
                    decimals_ = 2, n.inds_ = 55)
print(xtable(aics2, digits=2, 
             caption="AICc's from different hazard models.",
             label="CTAICc2"))
@

\todo{Should I maybe redo simulation and try get a scenario where the AICc prefers the season-specific hazards?}

\begin{figure}[ht]
\caption{\small Estimated encounter rates from the model with a common function for both seasons and from the model with season-specific functions.} 
\centering
\includegraphics[width=\textwidth]{keepfigure/SeasonalHazs.pdf}
\label{fig:seasonalhaz}
\end{figure}

Figure \ref{fig:seasonalhaz} shows that the estimated detection hazard in the wet season is higher during the day compared to the hotter dry season which is plausible. However the AICc's suggest that a common hazard is a better choice for this data. It is evident from the plot that the two hazards are not sufficiently different to justify estimating a bunch of extra parameters. Figure \ref{} shows the actual detection hazards that were used to simulate the data.

\begin{figure}[ht]
\caption{\small Actual detection hazards used to simulate the data used in this example.} 
\centering
\includegraphics[width=\textwidth]{keepfigure/TrueHazs.pdf}
\label{fig:seasonalhaz2}
\end{figure}


\section{Single-catch traps}
\label{sec: CT SC}
\index{}

\section{A single-catch trap application}
\label{sec: CT SC App}
\index{}



