\chapter{Continuous-time SCR models}
\label{chap:continuous-time}

\abstract{This chapter concerns models that use the observed times of capture rather than any occasion. These models facilitate a more natural modelling of data from devices like camera traps rather than needing to impose the artifical construct of occasions on to the data. }

\section{Introduction}
\label{sec:CT intro}
\index{}

Spatial and non-spatial application of classic CR typically involves physically capturing and marking individuals at discrete sampling occasions in time, and the survey process generates well-defined sampling occasions. But as discussed in Chapter~\ref{chap:det-types}, these days it is possible to use non-traditional detectors that do not involve any physical capture and release (hence the term \textit{passive} detectors) but rather record the actual detection times. 

Data of this sort do not lead to well-defined sampling occasions and researchers that have time-to-detection data typically aggregate their data into sampling occasions so that the data are then suitable for analysis with traditional models. Aggregating the data into discrete capture occasions imposes an artificial construction on the data for reasons of analytic convenience. It is always a better policy to find a statistical formulation that fits the data than to shoehorn the data into a form that fits statistical formulations that happen to be available. Furthermore aggregating data into capture occasions introduces subjectivity (the occasion length chosen by the analyst) and discards some information (the exact times of capture). 

As discussed previously, SCR methods assume independence between traps, i.e.\ the probability of being caught at a particular trap is not affected by a capture elsewhere. In general, the only benefit to imposing the artificial construct of occasions on continuous data is to remove correlation in the capture times so that the independence assumption is not violated. There is also likely to be little additional information in multiple captures in a short time \citep{Royle09, Royle09b}.

However, binning the data into occasions does not allow the inclusion of continuous covariates (for example weather) and limits the potential inferences that can be made regarding underlying biological processes that occur in continuous-time (CT).  Furthermore, the use of occasions often leads to the assumption of equal occasion lengths so that the interpretation of the capture probabilities are comparable across different periods. Such an assumption is questionable when factors like trap failure exist. It is common to aggregate the data into 24 hour periods \citep{Maffei04, Foster12} which can lead to the so called ``midnight problem'' \citep{Jordan11} where an animal photographed either side of midnight counts as two encounters as opposed to only counting as one encounter when captured twice during the same day. If animals are most active around the cutoff then capture probability will be positively biased, and this is likely to be true for nocturnal cats. Capture histories that use daily occasions for low density animals also tend to have many zeros \citep{Foster12}.

\section{Modelling time-to-detection data}
\label{sec: modelling CT data}
\index{}

The CT framework involves modelling the process generating detections as a temporal Non-Homogeneous Poisson Process (NHPP) in which the events are detections. This approach assumes that the encounter rate for the $i$th individual at the $j$th detector at any time is independent of the individual's capture history up to that time. Note that the model is easily extended to be of type $M_b$ by estimating different detection levels before and after first capture.

In a CT formulation, the discrete-occasion expression for the probability of detecting individual $i$ at detector $k$ during an occasion is replaced by an encounter rate function as seen in Chapter~\ref{chap:ERdetfun}. The difference is that the traditional SCR models make an implicit assumption that the expected encounter rate is constant through time but here the encounter rate function is extended to depend on time in addition to distance ($\lambda(d_{ij},t)$. When the expected encounter rate depends on time it tends to be referred to as the ``hazard'' (from survival analysis literature with the hazard in this case being the hazard of detection rather than hazard of death) though note that the expected encounter rate and the detection hazard are in effect the same thing.

The time-dependent expected encounter rate for the $i$th individual and the $j$th trap at time $t$ is denoted as $\lambda(d_{ij},t ; \bm{\theta})$ and can depend on both space (in terms of the distance from the trap to the activity centre $d_{ij}(\bm{s}_i)$) and time while $\bm{\theta}$ is an unknown vector of hazard function parameters.

The ``survivor function'' for individual $i$ at trap $j$ over the whole survey (the probability of individual $i$ not being detected by the trap by time $T$) is 

\[ S_j(T, d_{ij};\bm{\theta})=\exp{\left(-\int_0^T \lambda_j(d_{ij},t ; \bm{\theta})\;dt\right) }\] 

and hence $1 - S_j(T, d_{ij} ;\bm{\theta})$ is the probability of detection in $(0,T)$.
Similarly the combined expected encounter rate over all traps at time $t$ is $\lambda_\cdot(t,\bm{s}_i;\bm{\theta})=\sum_{j=1}^J \lambda_j(d_{ij},t ; \bm{\theta})$, and the overall probability of detection in $(0,T)$ over all detectors is $p_\cdot(\bm{s}_i;\bm{\theta})=1-S_\cdot(T, \bm{s}_i;\bm{\theta})$, where $S_\cdot(T,\bm{s}_i;\bm{\theta})=\exp{\left(-\int_0^T \lambda_j(\bm{s}_i,t ; \bm{\theta}))\;dt\right)}$ is the overall survivor function. 

By considering $\bm{t}_{ij}$ to be a realisation of a NHPP in time, an expression for the probability density function (pdf) of $\bm{t}_{ij}$ is obtained. This is analogous to the probability of obtaining a capture history at detector $j$ for individual $i$ in a discrete-time formulation, but it is worth noting that both the length of $\bm{t}_{ij}$ (denoted as $\omega_{ij}$ \todo{I'm a little unsure about the notation because $\omega$ is also used to represent capture histories?}) and the times that it contains are random variables, whereas for a discrete-occasion survey the length of a capture history is fixed by the number of occasions used.

Using a result from Poisson process theory \cite[see][Theorem 2.1 on page 30, and Equation (2.13) on page 32]{Cook07} \todo{is the citation necessary?}, and assuming that, conditional on the activity centre location, the times of detections are independent, the detection times for individual $i$ at detector $j$ can be modelled as a NHPP with pdf:

\begin{align}
f_j(\bm{t}_{ij} \mid \bm{s}_i ; \bm{\theta})
&= S_j(T,\bm{s}_i;\bm{\theta})\prod_{r=1}^{\omega_{ij}} \lambda_j(t_{ikr},\bm{s}_i;\bm{\theta})
\label{eqn:f_j(t)}
\end{align}

Figure~\ref{fig:Times1} shows a heuristic explanation of where this expression comes from for a hypothetical individual that is captured at a particular trap twice. It is necessary to first model the probability that the individual is not captured from the start of the survey until the time of first capture, and this probability is found by $\exp{-\int_0^{t_1} \lambda(t) \; dt}$. The individual is then captured at the appropriate encounter rate $\lambda(t_1)$. Next the probability of no capture from $t_1$ until $t_2$ is found in the same way and the second capture occurs at the rate of $\lambda(t_2)$. Lastly, it is necessary to model the fact that the individual evades capture from $t_2$ until the end of the survey at time $T$. 

\begin{figure}[ht]
\caption{\small The plot depicts a time-dependent hazard (for a given distance) and shows how two captures (at times $t_1$ and $t_2$) are modelled when they are treated as a NHPP. Multiplying these terms together leads to the expression $\exp{-\int_0^T \lambda(t)\; dt \prod_r \lambda(t_r)}$.}
\centering
\vspace{-24pt}
\includegraphics[width=12cm]{keepfigure/ModellingTimes.pdf}
\label{fig:Times1}
\end{figure}

Equation~\ref{eqn:f_j(t)} can be fitted into the alternative formulation presented in Section~\ref{sec:det-types-blocks}. The marginal distribution of the detection frequency at detector $j$ ($\omega_{ij}$) has a Poisson distribution with parameter $\Lambda_j(\bm{s}_i;\bm{\theta})=\int_0^T \lambda_j(t,\bm{s}_i;\bm{\theta})\;dt$, and the conditional pdf of detection times $\bm{t}_{ij}$, given $\omega_{ij}$, for the $i$th animal is

\begin{align}
f_{t|\omega,j}(\bm{t}_{ij} \mid \omega_{ij},\bm{s}_i;\bm{\theta})
&=\omega_{ij}!\prod_{r=1}^{\omega_{ij}}\frac{h_j(t_{ijr}, \bm{s}_i;\bm{\theta})}{\Lambda_j(\bm{s}_i;\bm{\theta})}.
\label{eq:f_j(t|omega)}
\end{align}

\todo[inline]{Not sure if I should show the full LL - the reason I was considering this is to discuss sufficiency and show that in a general sense at least there is information in the times. Just seems a bit too technical relative to the rest of the book.}

\section{Parameterising the expected encounter rate function}
\label{sec:CT hazards}
\index{}

When the hazard function varies through time, the hazard of individual $i$ being detected at trap $j$ depends as before on the distance between trap $j$ and individual $i$'s activity centre, but also depends on time. This scenario is more realistic as most animals are more likely to be caught at certain times rather than others depending on their behaviour and activity patterns.  

The detection hazards are conceived as cyclical hazards that repeat over time after a specified cycle length. This approach facilitates the modeling of a daily pattern to detectability that, for example, can result in nocturnal animals being very unlikely to be captured during the day. Using a cyclical hazard means that the hazard is constrained to begin and end at the same point each cycle. Note that the CT hazard results in a model in which variation in detectability is modelled at a finer resolution than DT models with occasions. 

\subsection{Seperating time and distance}
\label{subsec:independent}

One way to parametrise the time-dependent expected encounter rate function is to have a component that depends on distance ($\lambda(d ;\bm{\theta})$) and a component that depends on time ($h(t ; \bm{\psi})$). This parameterisation effectively means that these two components do not interact and hence the effect of time is independent of the effect of distance (and vica versa). 

\begin{equation}
\lambda(t, d ;\bm{\theta}, \bm{\psi}) = \lambda(d ;\bm{\theta}) \times h(t ; \bm{\psi}) 
\label{eqn:hazt1}
\end{equation}

The second component $h(t, \bm{\psi})$ determines the shape of the detectiomn hazard over time which is scaled by a factor that depends on the distance from the individuals' activity centre to the detector ($\lambda(d ;\bm{\theta})$). This scaling factor depends on the parameters that appear in the usual detection function, i.e.\ for the half-normal form the parameters in $\bm{\theta}$ include detectability ($g_0$) and dispersion characteristics ($\sigma$). 

Regression splines are a form of less rigid parametric fitting that can be thought of as fitting piecewise polynomials to segments of the data. A regression spline takes the form of a weighted sum of smooth functions of the data, and these functions of the data are known as basis functions. The flexibility of regression splines increases with the degrees of freedom or knots, i.e.\ increasing the knots increases the number of parameters that need to be estimated and increases the function's flexibility. 

An array of different types of basis functions are available that produce splines with different features. Cyclic cubic regression splines are used here to produce hazard cycles of a specified length that repeat over time.

The $h(t ; \bm{\psi})$ component with a flexible shape is parameterised as follows:

\begin{equation}
h(t ;\bm{\psi}) = e^{f(t ; \bm{\psi})}
\label{eqn:spline}
\end{equation}
 
where $f(t ; \bm{\psi}) = \beta_1 b_1(t) + \beta_2 b_2(t) + \ldots + \beta_p b_p(t) $, the $b_p(t)$ refer to $p$ basis functions, $p$ is determined by the specified degrees of freedom (denoted as $K$), and the log link ensures a positive detection hazard. The R package \emph{mgcv} \citep{Wood14} is used to construct the necessary basis functions. Specifying ``fit = FALSE'' in the \emph{gam} function returns the basis functions that can then be used as ordinary covariates in the hazard model. Note that the intercept of the spline coefficients is redundant because its role is effectively subsumed by the $\lambda(d ; \bm{\theta})$ scaling factor. Plots that depict the detection hazards are focusing on the shape through time and hence are scaled so that they integrate to 1. Furthermore, a degree of freedom is lost when specifying hazards with a cyclic cubic spline form. The number of degrees of freedom for different spline hazards that are reported here matches the number given to the \emph{gam} function in package \emph{mgcv}. Therefore the actual number of parameters estimated in the reported models is two less than the value given for $K$. 

Figure \ref{fig:GAMhaz} depicts three fictitious examples of different possible hazard shapes that can be modelled with regression splines. The top panel shows a simple detection hazard that rises to a peak about halfway through the cycle and then falls away again. The second example is similar but remains around its peak for a longer time. The final example shows a hazard that has two peaks, if one considers the 24 hour hazard cycle to be in sync with a 24 hour daily cycle then these peaks occur roughly at dawn and dusk. These shapes are produced by making up a series of data points that follow a certain pattern, extracting the basis functions, and then using the basis functions as covariates for fitting a generalised linear model with a log link. For example the fitted model for the hazard shape for the first shape with K = 4 is $h(t ;\bm{\psi}) = e^{1.145 b_1(t) + 1.284 b_2(t)}$. 

\begin{figure}[ht]
\caption{\small Three examples of possible detection hazard shapes that can be modelled using regression splines of increasing complexity. From top to bottom the degrees of freedom are 4, 8 and 10.}
\centering
\vspace{-24pt}
\includegraphics[width=12cm]{keepfigure/GAMhaz.pdf}
\label{fig:GAMhaz}
\end{figure}

\todo[inline]{something about ancillarity and density? Again not sure if too technical. COuld maybe leave out the maths and just say in a paragraph that the above formulation leads there being no information about density in the capture times. Which leads into the section below nicely.}

\subsection{A more general formulation}
\label{subsec:generalform}

The expected encounter rate function for a particular detector can be specified so that the effect of time depends on the distance between the trap and an individual's activity centre. One approach that achieves this is to make the $\sigma$ parameter depend on time. This leads to a model where the extent that an animal ranges depends on the time of day.

\begin{equation}
lambda(d,t) = \lambda_0\exp\{-d^2/(2\sigma(t)^2)\}
\label{eqn:hazt2}
\end{equation}
where $\sigma(t)=\exp(\beta_0 + \sum_{k=1}^K\beta_k\times b_k(t))$

\subsection{Linking with the detection function}
In Chapter~\ref{chap:ERdetfun}, Equation~\ref{eq:ER+detfun.binaryp} is used to link the detection function with the encounter rate model. However in that case an implicit assumption made is that the encounter rate is constant with time (which is why $\int_0^T \lambda(d_{ij}) \; dt = \lambda(d_{ij}) T$). 

The same approach is used to link the time-dependent encounter rate function with the detection function but it is generalised here. If $T_k = t_k-t_{k-1}$ where $t_{k-1}$ is the start of occasion $k$ and $t_k$ the end, then this is accomplished by specifying the detection probability for detector $j$ in occasion $k$ of duration $T_k$ in terms of the encounter rate function as follows. 

\begin{equation}
p(d_{ij})=1-\exp{\left(-\int_{t_{k-1}}^{t_k} \lambda_{j}(d,u) \;du \right)} 
\label{eqn:hazlink}
\end{equation} 

The paramaterisation used in Section~\ref{subsec:independent} means that $\int \lambda(d,t) \; dt = \lambda(d) \int h(t) \; dt$ and hence

\begin{equation}
\lambda_k(d) = \frac{-log(1-p_{jk}(d))}{ \int_{T_k} h(t)\;dt} 
\label{eqn:hazlink2}
\end{equation}

The more general formulation (Section~\ref{subsec:generalform}) does not allow this simplification and hence in that case:

\begin{equation}
\int \lambda_k(d,t) \; dt = -log(1-p_{jk}(d)) 
\label{eqn:hazlink2}
\end{equation}

This shows that there are many different encounter rate functional forms that can lead to the same detection function.

\section{Accounting for varying effort}
CT models readily accommodate detectors that are operational for different periods of time by setting the detection hazard at a detector to zero while the detector is out of operation (i.e.\ $h_j(t,\bm{s};\bm{\theta}, \bm{\psi})=0$ if detector $k$ was not operating at time $t$). 

This is the basis of the varying effort model for binary detectors and counts in \citet*{Efford13b}. However it should be noted that in that case it was assumed that the effect on the hazard is proportional to a standard unit of effort, i.e.\ if the effort for a given occasion is twice that of a standard unit of effort then the detection hazard gets doubled. The varying effort model can therefore accommodate occasions that are of different lengths but in order to do this it implicitly assumes that the hazard is constant through time. 

The CT framework generalises this idea and can for example recognise that a detector failing for a period of time during the day will have little effect on the detectability of a nocturnal species. The ability of the CT model to properly handle varying trap exposure is what leads to the construction of the single-catch trap likelihood. It is more difficult to accommodate detectors when their time of failing is unknown, and that problem is not addressed here. The assumption of a constant encounter rate through time is also made when the expected encounter rate for any duration $T$ is scaled in proportion to $T$, and also for the discussion on effort in X. Generalise effort idea from chapter 4.

\todo[inline]{Can put in a little more maths to show how shape of det fn is no longer normal for different durations?}

\todo[inline]{perhaps explore negative exponential parametrisation for lambda (see 1.19) rather than making it a function of g0 and sigma?}

\todo[inline]{link potential benefit of continuous covariates back to chapter 4's discussion on behavioural effects that disappear.}

\section{Learning about animal activity patterns}
\label{sec: activity}
\index{}

Understanding the activity patterns of animals can be used to learn about animal behaviour. \citet{Cowan12} use single-catch traps with timing devices to understand the behavioural response of possums that are caught, and \citet{Harmsen09} make inference about the hourly activity of jaguars and pumas from the frequency distribution of hourly capture times (397 for jaguars and 413 for pumas). However using a histogram with $X$ bars (a) requires arbitrary decisions about where the intervals start and end for each bar, (b) involves estimating ($X-1$) parameters, (c) does not give a smooth estimate of activity pattern change over time, and (d) does not include a rigorous framework for quantifying the associated uncertainty or selecting between alternative models. Using a spline involves no arbitrary decisions about intervals, involves estimating as many or as few parameters as the data support, provides a much more realistic smooth estimate of activity pattern change and a rigorous framework for model selection, uncertainty quantification, and testing hypotheses about activity patterns. The various hazard plots generated from the simulation studies in this chapter show that the CT models are able to estimate the underlying detection hazard cycle well. As one would expect, the variability in these estimates is higher for more complicated hazards that require more parameters, and decreases with more data. 

\citet{Ridout09} quantify the overlap in activity patterns between different species from camera trap data. They use kernel density estimation for circular data to estimate the continuous activity pattern distributions, and then look at different measures of overlap. The measures of overlap used in \citet{Ridout09} could also be applied to the estimated activity patterns from the estimated spline hazards. 

A crepuscular (active during twilight) hazard shape is deemed most appropriate for modelling detection of jaguars in the Cockscomb Basin Wildlife Sanctuary in Belize. This is in line with \citet{Harmsen11} who reported a crepuscular peak in activity for jaguars between 6 and 7 pm. On the other hand the estimated detection cycle for possums is more unimodal with a peak in the evening and low levels of detectability at other times, which is again in line with previously published work \citep{Cowan12}.  

A fuller moon leads to brighter hunting conditions and \citet{Harmsen11} report that the two main prey species of jaguars and pumas both exhibited reduced activity under these conditions, and discuss different potential predator strategies in response to this reduction in activity. The asynchronous hazard with a cycle duration of 720 hours is the type of hazard that could be appropriate if detectability is related to a longer process such as the lunar cycle.  However it is likely that if such a process existed it would be on top of a shorter daily detection cycle and  both would probably need to be modelled together. The objective in this case is to explore how the misspecification of the binary model affects density estimation and so the combination of both hazard cycles is not implemented here. The CT modelling framework can however easily accommodate this sort of hazard model.

\section{Modelling heterogeneity in detection}
\label{sec: CT hetero}
\index{}


\section{A camera trap application}
\label{sec: CT Cam App}
\index{}

\section{Single-catch traps}
\label{sec: CT SC}
\index{}

\section{A single-catch trap application}
\label{sec: CT SC App}
\index{}



