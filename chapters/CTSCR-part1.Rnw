\chapter{Continuous-time SCR models}
\label{chap:continuous-time}

\abstract{This chapter introduces models that use the observed times of capture and hence remove the need to impose an occasion structure on data collected in continouous time.  }

\section{Introduction}
\label{sec:CT intro}
\index{}

Spatial and non-spatial application of classic CR typically involves physically capturing and marking individuals at discrete sampling occasions in time, and the survey process generates well-defined sampling occasions. But as discussed in Chapter~\ref{chap:det-types}, these days it is possible to use non-traditional detectors that do not involve any physical capture and release (hence the term \textit{passive} detectors), in which case there are no well-defined sampling occasions.

Researchers using passive detectors typically aggregate their data into sampling occasions so that the data are then suitable for analysis with traditional models. However devices like camera traps are able to record the actual times of capture and aggregating time-to-detection data into discrete capture occasions imposes an artificial construction on the data for reasons of analytic convenience. It is always a better policy to find a statistical formulation that fits the data than to shoehorn the data into a form that fits statistical formulations that happen to be available. Furthermore, aggregating data into capture occasions introduces subjectivity (the occasion length chosen by the analyst) and discards some information (the exact times of capture). 

As discussed previously, SCR methods assume independence between traps, i.e.\ the probability of being caught at a particular trap is not affected by a capture elsewhere. In general, the only benefit to imposing the artificial construct of occasions on continuous data is to remove correlation in the capture times so that the independence assumption is not violated. There is also likely to be little additional information in multiple captures in a short time \citep{Royle09, Royle09b}.

However, binning the data into occasions does not allow the inclusion of continuous covariates (for example weather) and limits the potential inferences that can be made regarding underlying biological processes that occur in continuous-time (CT).  Furthermore, the use of occasions often leads to the assumption of equal occasion lengths so that the interpretation of the capture probabilities are comparable across different periods. Such an assumption is questionable when factors like trap failure exist. It is common to aggregate the data into 24 hour periods \citep{Maffei04, Foster12} which can lead to the so called ``midnight problem'' \citep{Jordan11} where an animal photographed either side of midnight counts as two encounters as opposed to only counting as one encounter when captured twice during the same day. If animals are most active around the cutoff then capture probability will be positively biased, and this is likely to be true for nocturnal cats. Capture histories that use daily occasions for low density animals also tend to have many zeros \citep{Foster12}.

The CT hazard results in a model in which variation in detectability is modelled at a finer resolution than DT models with occasions.\todo{need to find best place for this statement} 

\todo[inline]{link potential benefit of continuous covariates back to chapter 4's discussion on behavioural effects that disappear.}

\section{Modelling time-to-detection data}
\label{sec: modelling CT data}
\index{}

The CT framework involves modelling the process generating detections as a temporal Non-Homogeneous Poisson Process (NHPP) in which the events are detections. This approach assumes that the encounter rate for the $i$th individual at the $j$th detector at any time is independent of the individual's capture history up to that time. Note that the model is easily extended to be of type $M_b$ by estimating different detection levels before and after first capture.

In a CT formulation, the discrete-occasion expression for the probability of detecting individual $i$ at detector $j$ during occasion $k$ is replaced by an encounter rate function in the same way as seen in Chapter~\ref{chap:ERdetfun}. The difference is that the traditional SCR models make an implicit assumption that the expected encounter rate is constant through time but here the encounter rate function is extended to depend on time in addition to distance. Consequently the expected encounter rate for the $i$th individual and the $j$th trap at time $t$ now depends on both space (in terms of the distance from the trap to the activity centre $d_{ij}$) and time and is denoted as $\lambda(d_{ij},t)$.

Note that the survival analysis literature tends to refer to a time-dependent encounter rate as a ``hazard''. While these two terms can be used interchangeably the convention followed here is to use the term ``hazard'' when specifically referring to the time-dependent component of the encounter rate function, with the hazard in this case being
the hazard of detection rather than hazard of death.

The probability of individual $i$ not being detected by trap $j$ over the whole survey $T$ (also called the ``survivor function'') is 

\[ S_j(T, d_{ij})=\exp{\left(-\int_0^T \lambda_j(d_{ij},t)\;dt\right) }\] 

and hence $1 - S_j(T, d_{ij})$ is the probability of detection in $(0,T)$.

Similarly the combined expected encounter rate over all traps at time $t$ is $\lambda_\cdot(t,\bm{d}_i)=\sum_{j=1}^J \lambda_j(d_{ij},t)$ where the vector $\bm{d}_i=(d_{i1},\ldots,d_{iJ})$ represents distances to all detectors. The overall probability of detection in $(0,T)$ over all detectors is therefore $p_\cdot(\bm{d}_i)=1-S_\cdot(T, \bm{d}_i)$, where $S_\cdot(T,\bm{d}_i)=\exp{\left(-\int_0^T \lambda_\cdot(\bm{d}_i,t))\;dt\right)}$ is the overall survivor function. 

By considering the vector of capture times for the $i$th individual at the $j$th detector $\bm{t}_{ij}$ to be a realisation of a NHPP in time, an expression for the probability density function (pdf) of $\bm{t}_{ij}$ is obtained. This is analogous to the probability of obtaining a capture history at detector $j$ for individual $i$ in a discrete-time formulation, but it is worth noting that both the length of $\bm{t}_{ij}$ (denoted as $\omega_{ij}$ \todo{I'm a little unsure about the notation because $\omega$ is also used to represent capture histories?}) and the times that it contains are random variables, whereas for a discrete-occasion survey the length of a capture history is fixed by the number of occasions used.

Using a result from Poisson process theory \cite[see][Theorem 2.1 on page 30, and Equation (2.13) on page 32]{Cook07} \todo{is the citation necessary?}, and assuming that, conditional on the activity centre location, the times of detections are independent, the detection times for individual $i$ at detector $j$ can be modelled as a NHPP with pdf:

\begin{align}
f_j(\bm{t}_{ij} \mid d_{ij})
&= S_j(T,d_{ij})\prod_{r=1}^{\omega_{ij}} \lambda_j(d_{ij},t_{ikr})
\label{eqn:f_j(t)}
\end{align}

A heuristic explanation of this expression for a hypothetical individual that is captured at a particular trap twice is given in  Figure~\ref{fig:Times1}. It is necessary to first model the probability that the individual is not captured from the start of the survey until the time of first capture, and this probability is found by $\exp{-\int_0^{t_1} \lambda_j(d_{ij},t) \; dt}$. The individual is then captured at the appropriate encounter rate $\lambda_j(d_{ij},t_1)$. Next the probability of no capture from $t_1$ until $t_2$ is found in the same way and the second capture occurs at the rate of $\lambda_j(d_{ij},t_2)$. Lastly, it is necessary to model the fact that the individual evades capture from $t_2$ until the end of the survey at time $T$. 

\begin{figure}[ht]
\caption{\small The plot depicts a time-dependent hazard (for a given distance) and shows how two captures (at times $t_1$ and $t_2$) are modelled when they are treated as a NHPP. Multiplying these terms together leads to the expression $e^{-\int_0^T \lambda_j(d_{ij},t)\; dt} \prod_r \lambda_j(d_{ij},t_{ijr})$.}
\centering
\includegraphics[width=12cm]{keepfigure/ModellingTimes.pdf}
\label{fig:Times1}
\end{figure}

Equation~\ref{eqn:f_j(t)} can be fitted into the alternative formulation presented in Section~\ref{sec:det-types-blocks}. The marginal distribution of the detection frequency at detector $j$ for the $i$th animal ($\omega_{ij}$) has a Poisson distribution with parameter $\Lambda_j(d_{ij})=\int_0^T \lambda_j(d_{ij},t)\;dt$, and the conditional pdf of detection times $\bm{t}_{ij}$, given $\omega_{ij}$ is

\begin{align}
f(\bm{t}_{ij} \mid \omega_{ij},d_{ij})
&=\omega_{ij}!\prod_{r=1}^{\omega_{ij}}\frac{\lambda_j(d_{ij}, t_{ijr})}{\Lambda_j(d_{ij})}.
\label{eq:f_j(t|omega)}
\end{align}

\todo[inline]{Not sure if I should show the full LL - the reason I was considering this is to discuss sufficiency and show that in a general sense at least there is information in the times. Just seems a bit too technical relative to the rest of the book.}

\section{Parameterising the expected encounter rate function}
\label{sec:CT hazards}
\index{}

When the encounter rate function varies through time, the expected encounter rate of individual $i$ being detected at trap $j$ depends as before on the distance between trap $j$ and individual $i$'s activity centre, but also depends on time. This scenario is more realistic as most animals are more likely to be caught at certain times rather than others depending on their behaviour and activity patterns. The $_{ij}$ notation is suppressed in this section for brevity.   

The time-dependent detection hazard is conceived here as a cyclical hazard that repeats over time after a specified cycle length. This approach facilitates the modeling of a daily pattern to detectability that, for example, can result in nocturnal animals being very unlikely to be captured during the day. Using a cyclical hazard means that the hazard is constrained to begin and end at the same point each cycle. 

\subsection{Seperating time and distance}
\label{subsec:independent}

One way to parameterise the expected encounter rate function is to have a component that depends on distance ($\lambda(d)$) and a component that depends on time ($h(t)$). This parameterisation effectively means that these two components do not interact and hence the effect of time is independent of the effect of distance (and vica versa). 

\begin{equation}
\lambda(d,t) = \lambda(d) \times h(t) 
\label{eqn:hazt1}
\end{equation}

The second component $h(t)$ determines the shape of the detection hazard over time and the resulting time-dependent hazard of detection is scaled by $\lambda(d)$ that depends on the distance from the individuals' activity centre to the detector. 

%This scaling factor depends on the parameters that appear in the usual detection function, i.e.\ for the half-normal form the parameters in $\bm{\theta}$ include detectability ($g_0$) and dispersion characteristics ($\sigma$). 

A flexible way to model $h(t)$ is to use regression splines. Regression splines are a form of less rigid parametric fitting that can be thought of as fitting piecewise polynomials to segments of the data. A regression spline takes the form of a weighted sum of smooth functions of the data, and these functions of the data are known as basis functions. The flexibility of regression splines increases with the degrees of freedom or knots, i.e.\ increasing the knots increases the number of parameters that need to be estimated and increases the function's flexibility. 

An array of different types of basis functions are available that produce splines with different features. Cyclic cubic regression splines are used here to produce hazard cycles of a specified length that repeat over time.

The $h(t)$ component with a flexible shape is therefore parameterised as follows:

\begin{equation}
h(t) = e^{f(t)}
\label{eqn:spline}
\end{equation}
 
where $f(t) = \beta_1 b_1(t) + \beta_2 b_2(t) + \ldots + \beta_p b_p(t) $, the $b_p(t)$ refer to $p$ basis functions, $p$ is determined by the specified degrees of freedom (denoted as $K$), and the log link ensures a positive detection hazard. 

The R package \emph{mgcv} \citep{Wood14} is used to construct the necessary basis functions that are then used as ordinary covariates in the hazard model. Note that the intercept of the spline coefficients is redundant because its role is effectively subsumed by the $\lambda(d)$ scaling factor and hence it is ommitted from Equation \ref{eqn:spline} above. Furthermore, a degree of freedom is lost when specifying hazards with a cyclic cubic spline form. The number of degrees of freedom ($K$) for different spline hazards that are reported in this chapter matches the number passed to the \emph{gam} function in package \emph{mgcv} and hence the actual number of parameters estimated in the reported models is two less than $K$.

Figure \ref{fig:GAMhaz} depicts three fictitious examples of different possible hazard shapes that can be modelled with regression splines. These plots focus on the shape of the detection hazard through time and hence are scaled so that they integrate to 1. The top panel shows a simple detection hazard that rises to a peak about halfway through the cycle and then falls away again. The second example is similar but remains around its peak for a longer time. The final example shows a hazard that has two peaks, if one considers the 24 hour hazard cycle to be in sync with a 24 hour daily cycle then these peaks occur roughly at dawn and dusk. These shapes are produced by making up a series of data points that follow a certain pattern, extracting the basis functions, and then using the basis functions as covariates for fitting a generalised linear model with a log link. 
For example the fitted model for the hazard shape with K = 4 that is shown in the top panel is $h(t) = e^{1.145 b_1(t) + 1.284 b_2(t)}$. 

\begin{figure}[ht]
\caption{\small Three examples of possible detection hazard shapes that can be modelled using regression splines of increasing complexity. From top to bottom the degrees of freedom (as passed to the \emph{gam} function in package \emph{mgcv}) are 4, 8 and 10.}
\centering
\includegraphics[width=12cm]{keepfigure/GAMhaz.pdf}
\label{fig:GAMhaz}
\end{figure}

\todo[inline]{something about ancillarity and density? Again not sure if too technical. Could maybe leave out the maths and just say something like what appears below:}

A consequence of seperating the effects of time and distance is that there is no information in the capture times about density. Therefore a discrete time model estimator that ignores the capture times will not be biased for density (providing that the underlying data is being generated from such a process).  

\subsection{A more general formulation}
\label{subsec:generalform}

The expected encounter rate function can be specified so that the effects of time and distance interact. This means that the effect of time on the expected encounter rate depends on the distance between the trap and an individual's activity centre (and vica versa). One way to achieve this is to make the $\sigma$ parameter depend on time. This leads to a model where the extent that an animal ranges depends on the time of day and so has a useful biological interpretation.

\begin{equation}
\lambda(d,t) = \lambda_0\exp\{-d^2/(2\sigma(t)^2)\}
\label{eqn:hazt2}
\end{equation}
where $\sigma(t)=\exp(\beta_0 + \sum_{k=1}^K\beta_k\times b_k(t))$

Figure \ref{fig:3DHaz1} depicts how such an encounter rate function depends on both time and distance. If the detector is very far away from an animal's activity centre, the animal will hardly be detectable, no matter what time of day it is. Similarly, it is apparent that at times of the day when an animal does not range far (and so has a low value for $\sigma$) the expected encounter rate falls away much more quickly with distance than at times that correspond to higher higher values for $\sigma$.  

Note that with this parameterisation the $\lambda_0$ parameter does not depend on time. Effectively this means that if a detector is placed right on top of an animal's activity centre the expected encounter rate will be constant through time, which explains why the function tends to a flat surface as distance tends to zero. An encounter rate function with this characteristic may be plausible for certain types of animals, for example if an animal tends to rest very close to its activity centre and even during resting it moves around a little and is hence still detectable \todo[inline]{not sure about this last statement?}. Of course it is possible to extend the flexibility of the function and allow $\lambda_0$ to also depend on time, or to explore an alternative sort of parameterisation similar to \citet{Efford14}.

\begin{figure}[ht]
\caption{\small The three dimensional figure depicts a hypothetical expected encounter rate function that is parameterised according to Equation \ref{eqn:hazt2}. It is apparent how the expected encounter rate through time depends on distance and vica versa. The plot shows one cycle of 24 hours. }
\centering
\includegraphics[width=12cm]{keepfigure/3DHazplot.pdf}
\label{fig:3DHaz1}
\end{figure}

\todo{A paragraph and perhaps a table that summarise simulation results that hopefully show that the DT estimator is biased if the underlying process is akin to this sort of hazard and that in such a case there is important information in the capture times.}

\subsection{Linking with the detection function}
In Chapter~\ref{chap:ERdetfun}, Equation~\ref{eq:ER+detfun.binaryp} is used to link the detection function with the encounter rate model. However in that case an implicit assumption made is that the encounter rate is constant with time (which is why $\int_0^T \lambda(d_{ij}) \; dt = \lambda(d_{ij}) T$). 

Here the same approach is used to link the time-dependent encounter rate function with the detection function but it is generalised for time-dependent hazards. If $T_k = t_k-t_{k-1}$ where $t_{k-1}$ is the start of occasion $k$ and $t_k$ the end, then the linkage is accomplished by specifying the detection probability for detector $j$ in occasion $k$ of duration $T_k$ in terms of the encounter rate function as follows. 

\begin{equation}
p_k(d)=1-\exp{\left(-\int_{t_{k-1}}^{t_k} \lambda_{j}(d,u) \;du \right)} 
\label{eqn:hazlink}
\end{equation} 

The paramaterisation used in Section~\ref{subsec:independent} means that $\int \lambda_j(d,t) \; dt = \lambda_j(d) \int h(t) \; dt$ and hence

\begin{equation}
\lambda_k(d) = \frac{-log(1-p_{jk}(d))}{ \int_{T_k} h(t)\;dt} 
\label{eqn:hazlink2}
\end{equation}

The more general formulation (Section~\ref{subsec:generalform}) does not allow this simplification and hence in that case:

\begin{equation}
\int \lambda_k(d,t) \; dt = -log(1-p_{jk}(d)) 
\label{eqn:hazlink2}
\end{equation}

There are therefore many different encounter rate functional forms that can lead to the same detection function.

\section{Accounting for varying effort}
CT models readily accommodate detectors that are operational for different periods of time by setting the expected encounter rate at a detector to zero while the detector is out of operation (i.e.\ $\lambda_j(d,t)=0$ if detector $j$ was not operating at time $t$). 

This is the basis of the varying effort model for binary detectors and counts in \citet*{Efford13b}. However it should be noted that in that case it was assumed that the effect on the hazard is proportional to a standard unit of effort, i.e.\ if the effort for a given occasion is twice that of a standard unit of effort then the detection hazard gets doubled. The varying effort model can therefore accommodate occasions that are of different lengths but in order to do this it implicitly assumes that the hazard is constant through time. The assumption of a constant encounter rate through time is also made when the expected encounter rate for any duration $T$ is scaled in proportion to $T$, and also for the discussion on effort in Section \ref{sec:ERdetfund-effort}.

The CT framework generalises this idea and can for example recognise that a detector failing for a period of time during the day may have little effect on the detectability of a nocturnal species. The ability of the CT model to properly handle varying trap exposure is what leads to the construction of the single-catch trap likelihood seen in Section \ref{sec: CT SC}. It is more difficult to accommodate detectors when their time of failing is unknown, and that problem is not addressed here.  

\todo[inline]{Can put in a little more maths to show how shape of det fn is no longer normal for different durations? Seems a bit technical which is why I have left this out.}

\section{Learning about animal activity patterns}
\label{sec: activity}
\index{}

Understanding the activity patterns of animals can be used to learn about animal behaviour. \citet{Cowan12} use single-catch traps with timing devices to understand the behavioural response of possums that are caught, and \citet{Harmsen09} make inference about the hourly activity of jaguars and pumas from the frequency distribution of hourly capture times (397 for jaguars and 413 for pumas). However using a histogram with $X$ bars (a) requires arbitrary decisions about where the intervals start and end for each bar, (b) involves estimating ($X-1$) parameters, (c) does not give a smooth estimate of activity pattern change over time, and (d) does not include a rigorous framework for quantifying the associated uncertainty or selecting between alternative models. Using a spline involves no arbitrary decisions about intervals, involves estimating as many or as few parameters as the data support, provides a much more realistic smooth estimate of activity pattern change and a rigorous framework for model selection, uncertainty quantification, and testing hypotheses about activity patterns. 

Simulations confirm that CT models are able to estimate the underlying detection hazard cycle well. As one would expect, the variability in these estimates is higher for more complicated hazards that require more parameters, and decreases with more data. 

\citet{Ridout09} quantify the overlap in activity patterns between different species from camera trap data. They use kernel density estimation for circular data to estimate the continuous activity pattern distributions, and then look at different measures of overlap. The measures of overlap used in \citet{Ridout09} could also be applied to the estimated activity patterns from the estimated spline hazards. 

\section{Modelling heterogeneity in detection}
\label{sec: CT hetero}
\index{}

So far the discussion 
A fuller moon leads to brighter hunting conditions and \citet{Harmsen11} report that the two main prey species of jaguars and pumas both exhibited reduced activity under these conditions, and discuss different potential predator strategies in response to this reduction in activity. The asynchronous hazard with a cycle duration of 720 hours is the type of hazard that could be appropriate if detectability is related to a longer process such as the lunar cycle.  However it is likely that if such a process existed it would be on top of a shorter daily detection cycle and  both would probably need to be modelled together. The objective in this case is to explore how the misspecification of the binary model affects density estimation and so the combination of both hazard cycles is not implemented here. The CT modelling framework can however easily accommodate this sort of hazard model.

\section{A camera trap application}
\label{sec: CT Cam App}
\index{}


A crepuscular (active during twilight) hazard shape is deemed most appropriate for modelling detection of jaguars in the Cockscomb Basin Wildlife Sanctuary in Belize. This is in line with \citet{Harmsen11} who reported a crepuscular peak in activity for jaguars between 6 and 7 pm. On the other hand the estimated detection cycle for possums is more unimodal with a peak in the evening and low levels of detectability at other times, which is again in line with previously published work \citep{Cowan12}.  

\section{Single-catch traps}
\label{sec: CT SC}
\index{}

\section{A single-catch trap application}
\label{sec: CT SC App}
\index{}



