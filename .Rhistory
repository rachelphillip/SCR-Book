}
if(length(x) != 3) stop("hobbs.res -- parameter vector n!=3")
y<-c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 38.558, 50.156, 62.948,
75.995, 91.972)
t<-1:12
res<-x[1]/(1+x[2]*exp(-1*x[3]*t)) - y
}
hobbs.f.scaled<-function(x){ # Unscaled Hobbs weeds problem -- function
if (abs(12*x[3]) > 50) { # check computability
fbad<-.Machine$double.xmax
return(fbad)
}
res<-hobbs.res.scaled(x)
f<-sum(res*res)
}
hobbs.res.scaled<-function(x){ # Unscaled Hobbs weeds problem -- residual
if (abs(12*x[3]) > 50) { # check computability
rbad<-rep(.Machine$double.xmax, length(x))
return(rbad)
}
if(length(x) != 3) stop("hobbs.res -- parameter vector n!=3")
y<-c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 38.558, 50.156, 62.948,
75.995, 91.972)
t<-1:12
res<- 100 * x[1]/(1+ 10* x[2]*exp(-1* 0.10 * x[3]*t)) - y
}
num.sims <- 100
sc.estimates <- NULL
un.estimates <- NULL
sc.num.succ <- 0
sc.conv <- vector(mode="numeric", length=num.sims)
sc.c1 <- vector(mode="numeric", length=num.sims)
sc.c2 <- vector(mode="numeric", length=num.sims)
un.num.succ <- 0
un.conv <- vector(mode="numeric", length=num.sims)
un.c1 <- vector(mode="numeric", length=num.sims)
un.c2 <- vector(mode="numeric", length=num.sims)
for (i in 1:num.sims) {
ran.start <- runif(3, min=0, max=5)
result <- optim(ran.start, hobbs.f.scaled, control=list(trace=0))
sc.conv[i] <- result$convergence
if (result$convergence == 0)  sc.estimates <- rbind(sc.estimates, result$par)
if (result$value < 2.6) {
sc.num.succ <- sc.num.succ + 1
sc.c1[i] <- result$counts[1]
sc.c2[i] <- result$counts[2]
}
result <- optim(ran.start, hobbs.f, control=list(trace=0))
un.conv[i] <- result$convergence
if (result$convergence == 0) un.estimates <- rbind(un.estimates, result$par)
if (result$value < 2.6) {
un.num.succ <- un.num.succ + 1
un.c1[i] <- result$counts[1]
un.c2[i] <- result$counts[2]
}
}
draw.unscaled <- function(x, vec) {
a <- vec[1]
b <- vec[2]
c <- vec[3]
return(a / (1 + b*exp(-c*x)))
}
# draw.unscaled(5, vec=un.estimates[3,], add=TRUE)
# curve(draw.unscaled, from=0, to=12, vec=un.estimates[3,],add=TRUE)
plot(t,y, main="Weed fit with unscaled problem")
for (i in 1:dim(un.estimates)[1]) {
lines(1:12, draw.unscaled(1:12, un.estimates[i,]))
}
plot(t,y, main="Population fit without scaling")
for (i in 1:dim(un.estimates)[1]) {
lines(1:12, draw.unscaled(1:12, un.estimates[i,]))
}
plot(t,y)
hobbs.grad
hobbs.grad <- deriv(y~a/(1+b*exp(-c*t)),
c("a", "b","c"),
function(t,a,b,c){})
hobbs.f<-function(x){ # Unscaled Hobbs weeds problem -- function
if (abs(12*x[3]) > 50) { # check computability
fbad<-.Machine$double.xmax
return(fbad)
}
res<-hobbs.res(x)
f<-sum(res*res)
}
hobbs.res<-function(x){ # Unscaled Hobbs weeds problem -- residual
if (abs(12*x[3]) > 50) { # check computability
rbad<-rep(.Machine$double.xmax, length(x))
return(rbad)
}
if(length(x) != 3) stop("hobbs.res -- parameter vector n!=3")
y<-c(5.308, 7.24, 9.638, 12.866, 17.069, 23.192, 31.443, 38.558, 50.156, 62.948,
75.995, 91.972)
t<-1:12
res<-x[1]/(1+x[2]*exp(-1*x[3]*t)) - y
}
opt.with.grad <- optim(c(1,1,1), fn=hobbs.f,
gr=attr(hobbs.grad,"gradient"), method = "BFGS")
summary(opt.with.grad)
opt.with.grad$par
opt.with.grad$convergence
opt.with.grad$coount
opt.with.grad$count
opt.with.grad$value
opt.without.grad <- optim(c(1,1,1), fn=hobbs.f, method = "BFGS")
?microbenchmark
library(microbenchmark)
?microbenchmark
microbenchmark(opt.without.grad <- optim(c(1,1,1), fn=hobbs.f, method = "BFGS"), times=100)
time.grad <- microbenchmark(opt.with.grad <-
optim(c(1,1,1), fn=hobbs.f, gr=attr(hobbs.grad,"gradient"), method = "BFGS"), time=1000)
time.nograd <- microbenchmark(opt.without.grad <-
optim(c(1,1,1), fn=hobbs.f, method = "BFGS"), times=1000)
time.grad <- microbenchmark(opt.with.grad <-
optim(c(1,1,1), fn=hobbs.f, gr=attr(hobbs.grad,"gradient"), method = "BFGS"), time=1000)
summary(time.grad)
summary(time.nograd)
time.grad <- microbenchmark(opt.with.grad <-
optim(c(1,1,1), fn=hobbs.f, gr=attr(hobbs.grad,"gradient"),
method = "BFGS"), times=1000)
time.nograd <- microbenchmark(opt.without.grad <-
optim(c(1,1,1), fn=hobbs.f, method = "BFGS"), times=1000)
View(time.grad)
bill <- summary(time.grad)
str(bill)
summary(time.nograd, 2:7)
summary(time.nograd)[, 2:7]
summary(time.grad)[,2:7]
?optim
rm(bill)
slice <- function(mic) return(summary(mic)[,2:7])
time.grad.nm <- microbenchmark(opt.with.grad <-
optim(c(1,1,1), fn=hobbs.f, gr=attr(hobbs.grad,"gradient"),
method = "Nelder-Mead"), times=1000)
time.nograd.nm <- microbenchmark(opt.without.grad <-
optim(c(1,1,1), fn=hobbs.f, method = "Nelder-Mead"), times=1000)
jo <- slice(time.grad.bfgs, time.nograd.bfgs)
class(time.nograd.nm)
ls(pattern = "^time")
jo <- data.frame(NULL)
for (ob in ls(pattern = "^time")) {
jo <- rbind(slice(ob))
}
for (ob in ls(pattern = "^time")) {
jo <- rbind(jo, slice(ob))
}
slice(time.grad.nm)
names(summary(time.nograd.bfgs)[,2:7])
names(summary(time.nograd)[,2:7])
?data.frame
jo <- data.frame(min=NULL, lq=NULL, mean=NULL, median=NULL, uq=NULL, max=NULL)
View(jo)
for (ob in ls(pattern = "^time")) {
jo <- rbind(jo, slice(ob))
}
for (ob in ls(pattern="^time")) slice(ob)
ls(pattern="^time")
slice(time.grad)
slice(time.grad.nm)
slice(time.nograd)
slice(time.nograd.nm)
opt.without.grad <-
optim(c(1,1,1), fn=hobbs.f, method = "Nelder-Mead")
opt.without.grad
x=seq(-7,10,length=200)
y1=dnorm(x,mean=0,sd=1)
plot(x,y1,type='l',lwd=2,col='red')
y2=dnorm(x,mean=3,sd=2)
lines(x,y2,type='l',lwd=2,col='blue')
cord.x <- c(-3.418345,seq(-3.418345,1.418345,0.01),1.418345)
cord.y <- c(0,dnorm(seq(-3.418345,1.418345,0.01),3,2),0)
polygon(cord.x,cord.y,col='blue',lty=0)
cord.x <- c(1.418345,seq(1.418345,4,0.01),4)
cord.y <- c(0,dnorm(seq(1.418345,4,0.01)),0)
polygon(cord.x,cord.y,col='red',lty=0)
surfaceplot()
?surface
?plot3d
?optim
mining <- read.table("C:\\Users\\eric\\Dropbox\\MT4113\\pract-2016-rmd\\practical6\\mining_data.txt")
init.values <- c(alpha=35, beta=1)
nls.fit<-nls(angle~alpha*(1-exp(-beta*x)),start=init.values, data=mining, trace=TRUE)
Methods of software development
======================================
incremental: true
Two contrasting approaches
===========================
incremental: true
Software design for statisticians
=======================
incremental: true
Adapting (recycling) existing code
=================
incremental: true
Programming style
==================================
class: tiny-code
incremental: true
- Implement principle of least privilege (even though R doesn't mandate this)
>"Premature optimisation is the source of programming evil" (Knuth or Hoare)
- working code is preferable to fast, broken code
- Avoid repetition in code
- Seek out redundant code and convert to modules
```{r, eval=FALSE}
for (i in 1:num.patients) {
if(Smoke[i]==0) {
print(paste("Subject ",i," a Non-smoker has Heartrate", Heartrate[i]))
} else {
print(paste("Subject ",i," a Smoker has Heartrate", Heartrate[i]))
}
}
mining <- read.table("C:\\Users\\eric\\Dropbox\\MT4113\\pract-2016-rmd\\practical6\\mining_data.txt")
init.values <- c(alpha=35, beta=1)
nls.fit<-nls(angle~alpha*(1-exp(-beta*x)),start=init.values, data=mining, trace=TRUE)
mining <- read.table("C:\\Users\\eric\\Dropbox\\MT4113\\pract-2016-rmd\\practical6\\mining_data.txt")
mining$x<-mining$width/mining$depth
init.values <- c(alpha=35, beta=1)
nls.fit<-nls(angle~alpha*(1-exp(-beta*x)),start=init.values, data=mining, trace=TRUE)
mining <- read.table("C:\\Users\\eric\\Dropbox\\MT4113\\pract-2016-rmd\\practical6\\mining_data.txt",
header=TRUE)
mining$x<-mining$width/mining$depth
init.values <- c(alpha=35, beta=1)
nls.fit<-nls(angle~alpha*(1-exp(-beta*x)),start=init.values, data=mining, trace=TRUE)
fitted <- predict(nls.fit, data=mining)
plot(mining$x, fitted)
plot(mining$x, fitted, type='b')
fitted <- predict(nls.fit, data=mining)
plot(mining$x, fitted, type='b')
mining
with(x, plot(x, angle, type='b'))
with(mining, plot(x, angle, type='b'))
with(mining, plot(x, angle, type='p'))
fitted <- predict(nls.fit, data=mining)
line(mining$x, fitted, lwd=2)
lines(mining$x, fitted, lwd=2)
with(mining, plot(x, angle, type='p'))
fitted <- predict(nls.fit, data=mining)
lines(mining$x, fitted, lwd=2)
with(mining, plot(x, angle, type='p'))
fitted <- predict(nls.fit, data=mining)
lines(mining$x, fitted, lwd=2)
mining[order(mining$x),]
mining <- mining[order(mining$x),]
mining <- read.table("C:\\Users\\eric\\Dropbox\\MT4113\\pract-2016-rmd\\practical6\\mining_data.txt",
header=TRUE)
mining$x <- mining$width/mining$depth
mining <- mining[order(mining$x),]
init.values <- c(alpha=35, beta=1)
nls.fit <- nls(angle~alpha*(1-exp(-beta*x)),start=init.values, data=mining)
with(mining, plot(x, angle, type='p'))
fitted <- predict(nls.fit, data=mining)
lines(mining$x, fitted, lwd=2)
fitted.fine <- predict(nls.fit, data=data.frame(x=seq(from=0, to=2.5, length=100)))
finer.x <- seq(from=0, to=2.5, length=100))
fitted.fine <- predict(nls.fit, data=finer.x)
lines(finer.x, fitted.fine, lwd=2, lty=2)
finer.x <- seq(from=0, to=2.5, length=100)
fitted.fine <- predict(nls.fit, data=finer.x)
lines(finer.x, fitted.fine, lwd=2, lty=2)
fitted.fine <- predict(nls.fit, data=data.frame(x=finer.x))
lines(finer.x, fitted.fine, lwd=2, lty=2)
finer.x <- data.frame(x=seq(from=0, to=2.5, length=100))
fitted.fine <- predict(nls.fit, data=finer.x)
lines(finer.x, fitted.fine, lwd=2, lty=2)
?predict.nls
fitted.fine <- predict(nls.fit, newdata=finer.x)
lines(finer.x, fitted.fine, lwd=2, lty=2)
lines(finer.x$x, fitted.fine, lwd=2, lty=2)
with(mining, plot(x, angle, type='p'))
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
segments(mining$x, angle, mining$x, fitted)
with(mining, plot(x, angle, type='p'))
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
segments(mining$x, mining$angle, mining$x, fitted)
with(mining, plot(x, angle, type='p'), main="Mining subsidence", sub="From Myers (1990)")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
with(mining, plot(x, angle, type='p'), main="Mining subsidence", sub="From Myers (1990)")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
?plot
with(mining, plot(x, angle, type='p'), main="Mining subsidence", sub="From Myers (1990)")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
with(mining, plot(x, angle, type='p'), main="Mining subsidence")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
with(mining, plot(x, angle, type='p'), main="Mining subsidence")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
with(mining, plot(x, angle, type='p'), main="Mining subsidence")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
with(mining, plot(x, angle, type='p'), main="Mining subsidence")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
mining <- read.table("C:\\Users\\eric\\Dropbox\\MT4113\\pract-2016-rmd\\practical6\\mining_data.txt",
header=TRUE)
mining$x <- mining$width/mining$depth
mining <- mining[order(mining$x),]   # note ordering
init.values <- c(alpha=35, beta=1)
nls.fit <- nls(angle~alpha*(1-exp(-beta*x)),start=init.values, data=mining)
with(mining, plot(x, angle, type='p'), main="Mining subsidence")
fitted <- predict(nls.fit)
lines(mining$x, fitted, lwd=2)
fitted.fine <- predict(nls.fit, newdata=finer.x, se.fit = TRUE)
fitted.fine <- predict(nls.fit, newdata=finer.x, se.fit = TRUE)
fake.ci <- c(0.9, 1.1) * fitted.fine
?matrix
fake.ci <- matrix(data=NA, nrow=length(fitted.fine), ncol=2)
fake.ci[] <- c(0.9, 1.1) * fitted.fine
View(finer.x)
fake.ci <- c(0.9, 1.1) * fitted.fine
fake.ci[] <- c(0.9, 1.1) * fitted.fine
segments(finer.x$x, fake.ci[,1], finer.x$x, fake.ci[,2])
fake.ci[] <- c(0.9, 1.1) * fitted.fine
fake.ci <- matrix(data=NA, nrow=length(fitted.fine), ncol=2)
fake.ci[] <- c(0.9, 1.1) * fitted.fine
segments(finer.x$x, fake.ci[,1], finer.x$x, fake.ci[,2])
with(mining, plot(x, angle, type='p'))
finer.x <- data.frame(x=seq(from=0, to=2.5, length=100))
fitted.fine <- predict(nls.fit, newdata=finer.x, se.fit = TRUE) # does not work
fake.ci <- matrix(data=NA, nrow=length(fitted.fine), ncol=2)
fake.ci[] <- c(0.9, 1.1) * fitted.fine
segments(finer.x$x, fake.ci[,1], finer.x$x, fake.ci[,2])
lines(finer.x$x, fitted.fine, lwd=2, lty=2)
with(mining, plot(x, angle, type='p'))
finer.x <- data.frame(x=seq(from=0, to=2.5, length=100))
fitted.fine <- predict(nls.fit, newdata=finer.x, se.fit = TRUE) # does not work
fake.ci <- matrix(data=NA, nrow=length(fitted.fine), ncol=2)
fake.ci[] <- c(0.9, 1.1) * fitted.fine
segments(finer.x$x, fake.ci[,1], finer.x$x, fake.ci[,2])
# lines(finer.x$x, fitted.fine, lwd=2, lty=2)
?par
par(mgp=2)
par(mgp=c(3,2,0)
)
source('~/.active-rstudio-document')
library(DSsim)
sim <- make.simulation(design.obj = make.design("point"))
check.sim.setup(sim)
sim <- run(sim) #Note default is to run it 100 times
summary(sim)
install_github(distancedevelopment/Dssim)
devtools::install_github(distancedevelopment/Dssim)
devtools::install_github("DistanceDevelopment/Dssim")
devtools::install_github("DistanceDevelopment/DSsim")
devtools::install_github("DistanceDevelopment/DSsim/DSsim")
source('~/.active-rstudio-document')
summary(sim)
?run
?make.simulation
makeup <- c(45, 19, 17)
barplot(height=makeup, names.arg = c("C", "R", "Fortran"))
library(microbenchmark)
f0 <- function() NULL
f1 <- function(a = 1) NULL
f2 <- function(a = 1, b = 1) NULL
f3 <- function(a = 1, b = 2, c = 3) NULL
f4 <- function(a = 1, b = 2, c = 4, d = 4) NULL
f5 <- function(a = 1, b = 2, c = 4, d = 4, e = 5) NULL
results <- microbenchmark(f0(), f1(), f2(), f3(), f4(), f5(), times = 10000)
View(results)
results
knitr::kable(results[2:7,])
knitr::kable(results)
knitr::kable(results[1:7,])
results
j <- results
j <- print(results)
j
knitr::kable(j)
library(microbenchmark)
f0 <- function() NULL
f1 <- function(a = 1) NULL
f2 <- function(a = 1, b = 1) NULL
f3 <- function(a = 1, b = 2, c = 3) NULL
f4 <- function(a = 1, b = 2, c = 4, d = 4) NULL
f5 <- function(a = 1, b = 2, c = 4, d = 4, e = 5) NULL
results <- microbenchmark(f0(), f1(), f2(), f3(), f4(), f5(),
times = 100000)
knitr::kable(print(results))
mtcars[32,11]
mtcars
microbenchmark(
"[32, 11]"      = mtcars[32, 11],
"$carb[32]"     = mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11, 32)]],
"[[11]][32]"    = mtcars[[11]][32])
microbenchmark(
"[32, 11]"      = mtcars[32, 11],
"$carb[32]"     = mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11, 32)]],
"[[11]][32]"    = mtcars[[11]][32], units=nanoseconds)
?microbenchmark
microbenchmark(
"[32, 11]"      = mtcars[32, 11],
"$carb[32]"     = mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11, 32)]],
"[[11]][32]"    = mtcars[[11]][32], unit="nanoseconds")
microbenchmark(
"[32, 11]"      = mtcars[32, 11],
"$carb[32]"     = mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11, 32)]],
"[[11]][32]"    = mtcars[[11]][32], unit="ns")
?.subset2
str(mtcars)
class(mtcars)
mtcars[1:10, 1:6]
print.microbenchmark
cars <- microbenchmark(
"[32, 11]"      = mtcars[32, 11],
"$carb[32]"     = mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11, 32)]],
"[[11]][32]"    = mtcars[[11]][32], unit="ns")
print(cars, signif = 1)
cars <- microbenchmark(
"[32, 11]"      = mtcars[32, 11],
"$carb[32]"     = mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11, 32)]],
"[[11]][32]"    = mtcars[[11]][32])
print(cars)
print(cars, signif=2)
print(cars, signif=3)
install.packages("webshot")
library(webshot)
library(profvis)
profvis({
fill.mat <- runif(10000*10000)
x <- matrix(fill.mat, nrow=10000, ncol=10000)
y <- colSums(x)
})
webshot::install_phantomjs()
# Generate data
times <- 4e5
cols <- 150
data <- as.data.frame(x = matrix(rnorm(times * cols, mean = 5), ncol = cols))
data <- cbind(id = paste0("g", seq_len(times)), data)
profvis({
data1 <- data   # Store in another variable for this run
# Get column means
means <- apply(data1[, names(data1) != "id"], 2, mean)
# Subtract mean from each column
for (i in seq_along(means)) {
data1[, names(data1) != "id"][, i] <- data1[, names(data1) != "id"][, i] - means[i]
}
})
rm(list=ls())
# Generate data
times <- 4e5
cols <- 150
data <- as.data.frame(x = matrix(rnorm(times * cols, mean = 5), ncol = cols))
data <- cbind(id = paste0("g", seq_len(times)), data)
profvis({
data1 <- data   # Store in another variable for this run
# Get column means
means <- apply(data1[, names(data1) != "id"], 2, mean)
# Subtract mean from each column
for (i in seq_along(means)) {
data1[, names(data1) != "id"][, i] <- data1[, names(data1) != "id"][, i] - means[i]
}
})
?seq_along
seq_len(5)
profvis::profvis({data1 <- data   # Store in another variable for this run
# Get column means
means <- apply(data1[, names(data1) != "id"], 2, mean)
# Subtract mean from each column
for (i in seq_along(means)) {
data1[, names(data1) != "id"][, i] <- data1[, names(data1) != "id"][, i] - means[i]})
profvis::profvis({data1 <- data   # Store in another variable for this run
# Get column means
means <- apply(data1[, names(data1) != "id"], 2, mean)
# Subtract mean from each column
for (i in seq_along(means)) {
data1[, names(data1) != "id"][, i] <- data1[, names(data1) != "id"][, i] - means[i]
}})
apply
as.matrix
View(data1)
?vapply
?scale
data2 <- scale(data1[,2:dim(data1)[2]], scale=FALSE)
rm(data)
data2 <- scale(data1[,2:dim(data1)[2]], scale=FALSE)
data2[1:3, 1:10]
scale
data3 <- cbind(data1[,1], data2)
data3[1:3, 1:10]
data1[1:5,1]
4e5
4e5+1
x <- seq(0, 1, by=.1)
x
x==.6
x <- seq(0, 1, by=.1,dp=7)
?options
options(digits=10)
x
?nls
install.packages(c("maptools", "Rcpp", "rgdal", "statmod"))
(25*45)/60
(15*45)/60
(20*45)/60
source("scrmlebook/inst/code/packages.R")
setwd("~/GitHub/SCR-Book")
source("scrmlebook/inst/code/packages.R")
install.packages("spatstat")
source("scrmlebook/inst/code/packages.R")
library(scrmlebook)
